\chapter{Discussion}\label{chap:discussion}

The research outlined in this thesis was motivated by exploring the notion of ``style'' in jazz improvisation. Style relates both to the particular musical language and vocabulary of different jazz performers and ensembles, as well as different jazz subgenres and historical eras. To approach this issue, we applied supervised learning techniques and statistical analysis to large databases of jazz performances transcribed into symbolic forms like MIDI ``piano rolls''. A secondary motivation was to develop resources (including pre-trained models, datasets, software, and online web applications) that can be taken advantage of in future research. 

Much of this work relied on a collection of annotated audio recordings by jazz rhythm sections (the Jazz Trio Database or \GLS{JTD}) that is described in Chapter~\ref{chap:jtd_tismir}. \GLS{JTD} contains 44.5 hours of jazz piano solos (1,294 recordings) by 34 different pianists annotated by an automated pipeline, with data provided for all performers in the trio. This pipeline consisted of two stages whereby the raw audio is first ``demixed'' using music source separation models to create isolated piano, bass, and drums signals: each signal is subsequently annotated using automatic transcription algorithms optimized for that instrument. The annotations include onset times for each performer, MIDI transcriptions for the piano soloist, and beat and downbeat transcriptions for the audio mixture. \GLS{JTD} fulfils a niche lacking in existing datasets insofar that, by including annotations for multiple performers, it enables interesting group-level musical features to be analysed and modelled. We are encouraged by recent applications of \GLS{JTD} to tasks such as solo piano performance detection \citep{Bradshaw2025} and anticipate that it will prove useful for a variety of other \GLS{MIR} tasks as well.

A recurring theme of the following chapters was to respond to a statement made at the turn of the 21st century (first given in Chapter \ref{chap:introduction}), that
\begin{quote}
    \centering
    there doesn't exist ... any form of notation, graphic representation, and computer analysis capable of satisfactorily registering the subtlety of [the musical processes in jazz, that] differ not just stylistically but also in terms of specific distinctions between groups and individuals~\citep[p. 192]{Berendt1976}.
\end{quote} More specifically, we wanted to consider whether recent advances in machine learning and audio signal processing now enable us to develop computational tools capable of capturing these stylistic subtleties. Across Chapters~\ref{chap:xai_rsi}--\ref{chap:mp_network} we described a variety of tools, addressing three core phenomena in jazz improvisation: personal musical style, rhythmic style, and ensemble performance style. Finally, Chapter \ref{chap:conditioned_gen} put everything together and introduced an end-to-end model capable of generating new improvisations in the style of particular jazz performers and subgenres.

\textbf{Chapter~\ref{chap:xai_rsi}} considered personal improvisation style --- the ``specific distinctions between individuals'' --- encompassing a performer's use of particular melodic lines and rhythmic phrasing, amongst other factors. Understanding exactly which of these elements best reflect and define their style can be difficult, however, and may act as a barrier to learning and appreciating this music. We used machine learning to address this issue by training supervised-learning models to identify twenty pianists from recordings contained in \GLS{JTD} and \GLS{PiJAMA} using different forms of input. We focussed on three types of models. The first model was trained on ``handcrafted'' feature sets relating to melody and harmony, while the second learned representations directly from an input. This chapter culminated with the introduction of a deep neural network architecture that achieved near state-of-the-art prediction accuracy (91\% accuracy for twenty classes) with a multi-input structure that allowed its predictions to be explained in terms of four fundamental musical domains (melody, harmony, rhythm, and dynamics). Our analyses of these models highlighted the relative importance of each domain in distinguishing jazz pianists and uncovered a variety of melodic and harmonic patterns associated with each performer.

\textbf{Chapter~\ref{chap:rhythm_rsos}} considered rhythm --- in particular, the distinctiveness of the rhythmic style of individual performers, which was shown to be highly important to our multi-input model, but not considered in detail in the previous chapter. We trained a separate supervised-learning model to identify the performer in a recording using a targeted set of nineteen rhythmic features derived from ethnographic and pedagogical literature on jazz. Despite the compact size of this model, it nonetheless achieved 59\% accuracy when identifying ten performers within the \GLS{JTD-300} subset of data introduced in Section \ref{sec:jtd_class_imbalance}. The most important features used by this model related to a performer's ``feel'' (ensemble synchronisation) and ``complexity'' (information density), with lesser importance given to those relating to ``interaction'' (modelled sensorimotor coupling). Interpreting the distribution of these features also revealed insights into the style of each performer that were generally consistent with the critical writing on their music. The rhythmic style of each musician changed relatively little over the duration of their career, and there were no apparent differences in rhythmic feature usage across recordings made in different historical eras.

\textbf{Chapter~\ref{chap:mp_network}} considered the style of different jazz ensembles --- the ``specific distinctions between groups''. Group jazz performance depends on the temporal coordination of action in order to be effective. However, studying the means by which this coordination is achieved in real-time performances may not yield meaningful insight into the differences between ensembles, especially when musicians optimise their timing simply to reduce the total asynchrony \citep[][see also Section~\ref{sec:rsos_feature_importance}]{Jacoby2021}. Instead, we used temporal latency to disrupt coordination by introducing a delay between when a musical sound is produced and when it is received. In a study of ten professional jazz duos, we found two strategies that involved: (1) one musician being led by the other, tracking the timings of the leader's performance, and (2) both musicians accommodating to each other, mutually adapting their timing. During networked performance, these two strategies favoured different sides of a trade-off between tempo synchrony and stability; in the absence of delay, both achieved similar outcomes. By further analysing how the musicians in each ensemble described their own performances, we were able to obtain insights into their improvisation style that would be difficult to acquire simply by modelling commercial recordings.

Finally, \textbf{Chapter~\ref{chap:conditioned_gen}} focussed on generative modelling --- training a supervised learning model models to \textit{generate} new music using autoregressive prediction, rather than understand existing music (as in the preceding chapters). We developed an automated method to link three existing datasets of transcribed jazz piano performances (including \GLS{JTD}) with high-quality subgenre annotations. We then used this metadata to condition a large music language model to produce jazz performances in the style of particular subgenres and performers. We have open-sourced both the metadata and the checkpoints of our generative model. The intention of this chapter was to wrap up the work of the entire thesis by using many of the datasets and approaches we have introduced, while also looking forward to the future of symbolic music modelling and music generation. 

Given the work conducted in the previous chapters, one conclusion is clear: computational modelling \textit{does} now appear to offer one way of studying the specific stylistic distinctions between groups and individuals in jazz. One way that it can do so is by providing a means to objectively test terms and concepts that appear in musicological literature. For example: that Oscar Peterson plays extremely dense musical passages (or Ahmad Jamal extremely sparse passages) might be clear to a knowledgeable listener (see Figure \ref{fig:rsos_pianist_density}), but it can equally be difficult to locate ``the most Peterson-like'' (or ``Jamal-like'') passage in these terms. We have shown how advances in audio signal processing enable complex features to be extracted from large datasets in ways that would have previously required painstaking manual analysis. By training supervised-learning models on these features, we can apply techniques from explainable artificial intelligence --- as well as bespoke methods designed specifically for jazz (Section~\ref{sec:rsi_factorised_architecture}) --- to unpick the importance of abstract qualities like ``swing'' and ``feel'' to the style of particular jazz performers and groups. Such insights not only help clarify long-standing debates in jazz scholarship, but also provide tools for exploring stylistic evolution, influence, and innovation within the genre.

A second conclusion here is how the insights obtained from good models can validate and extend accounts given by performers themselves. In every chapter, we demonstrated how our supervised-learning models can be interpreted in ways that appear to reinforce claims made by particular musicians or groups as to their own style of improvisation. One could argue that this is obvious: our models simply show that musicians ``do what they say they do''. But, from another perspective, this is highly interesting. Considering several of the comments by the musicians featured in Chapter~\ref{chap:mp_network} (see also Appendix~\ref{chap:mp_appendix}) --- and also in interviews with many of the pianists with performances in \GLS{JTD} \citep[e.g., those in][]{Sidran1992, Berliner1994, Monson1996} --- it is clear that jazz musicians are not always able (or willing) to describe their own performance style. We have shown how computational techniques can help capture the complexities of these styles. In this sense, a good model does not merely confirm existing views on jazz improvisation, but can also reveal deeper structures of musical thought and practice that might otherwise have remained inaccessible to researchers.

A second motivation for this work was to support future computational analysis of jazz improvisation with a range of open-source software packages, datasets, and web applications. Perhaps our most substantive contribution here is \GLS{JTD} itself, which is now a part of the widely used \texttt{mirdata} library \citep{Bittner2019}. This, we hope, should enable straightforward and reproducible experimentation by members of the \GLS{MIR} community. A variety of code ``recipes'' are given in Section \ref{sec:jtd_subsequent_developments} and in the online documentation for this dataset.\footnote{\url{https://huwcheston.github.io/Jazz-Trio-Database}} 

In addition, we have also developed several web applications relating to \GLS{JTD}. The first allows a variety of features extracted from its recordings to be explored in a browser,\footnote{\url{https://huwcheston.github.io/Jazz-Trio-Database/resources/data-explorer.html}} and the second provides an interactive graph of the playing networks between the different musicians (piano, bass, and drums) with recordings in \GLS{JTD-300}.\footnote{\url{https://huwcheston.github.io/Jazz-Trio-Database/resources/trio-network-search.html}} The models introduced in Chapter~\ref{chap:xai_rsi} are accompanied by an interactive web application that allows users to listen to examples of the melodic patterns associated with performers in \GLS{JTD} (Section~\ref{sec:rsi_handcrafted_feature_extraction}), as well as view the overall distinctive components of their improvisation style (Section~\ref{sec:rsi_factorised_domain_importance}).\footnote{\url{https://huwcheston.github.io/ImprovID-app/index.html}} The model presented in Chapter~\ref{chap:rhythm_rsos} is accompanied by another web application that plots the predictions of the model on an interactive map, such that recordings in \GLS{JTD} with inherent rhythmic similarities are placed closer together.\footnote{\url{https://huwcheston.github.io/Jazz-Trio-Database/_static/prediction-app.html}} 

As part of the work conducted in Chapter~\ref{chap:mp_network}, we have developed a software platform (described in Section~\ref{sec:mp_testbed}) capable of introducing controlled, deterministic manipulations (including --- but not limited to --- simulated network delay) into real-time music performances. We also compiled an additional dataset of four hours of MIDI, audio, and video recordings by professional jazz pianists and drummers that could be used to train downstream \GLS{MIR} models in tasks such as audio-to-video alignment. We also collected a dataset of user evaluations that could be used either in cognitive modelling or to fine-tune music language models to align with human preferences.\footnote{\url{https://doi.org/10.5281/zenodo.7773824}} We hope that these packages, web applications, and datasets will prove useful to members of the community hoping to conduct subsequent computational research into jazz improvisation. 

Finally, as part of the work conducted in Chapter~\ref{chap:conditioned_gen}, we have also linked \GLS{JTD} with high-quality metadata relating to different jazz subgenres. We used this information, as well as the dataset introduced in Chapter~\ref{chap:mp_network}, to condition a music language model to produce jazz piano performances in the style of particular musicians and jazz subgenres. We have since open-sourced this model and the associated metadata.\footnote{\url{https://doi.org/10.5281/zenodo.15610452}} We have also made several examples of performances generated using it available on an interactive web application.\footnote{\url{https://huwcheston.github.io/jazz-style-conditioned-generation/index.html}}

\section{Limitations and Future Directions}\label{sec:discussion_limitations_and_future_directions}

We can foresee several limitations of the work described in this thesis. Firstly, our research was restricted to modelling jazz improvisation style in the symbolic domain. Our reasoning was that symbolic representations such as MIDI ``piano rolls'' are widely used by musicians and thus lend themselves to meaningful interpretations --- over and above auditory representations such as spectrograms, which may not be as straightforward to understand \citep{Foscarin2022}. However, expressive jazz improvisation engages with a greater number of stylistic parameters than can be feasibly be captured by a symbolic representation. For instance, aspects of tone and timbre are part of the stylistic vocabulary of many jazz musicians \citep{Abeser2015-scoreinformed, Weis2018}. These factors would need to be considered if the methodology developed here were to be applied to a wider range of instruments, likely through the use of an audio-based (as opposed to symbolic) representation \citep[here, see particularly][]{Ramirez2010}.

We also did not consider the visual dimension in our work. This is important not only for allowing musicians to coordinate effectively with each other (\cite{Chang2017, Moran2015, Doffman2014}; see also several comments in Appendix~\ref{chap:mp_appendix}), but also --- through the use of particularly idiosyncratic bodily movements, for instance --- becomes a key part of the musical style of several jazz musicians considered here, such as Keith Jarrett \citep{Elsdon2013}. Ideally, the phase correction models described in Chapter~\ref{chap:mp_network} would be augmented with equivalent analysis of the video data that we gathered at the same time (see Section~\ref{sec:mp_testbed}). This would allow for a more substantive analysis of lead-lag phenomena along the lines of \citet{Chang2017} and \citet{Moran2015}. Incorporating video alongside audio can also prove useful in several \GLS{MIR} tasks related to performer identification, such as genre and style classification \citep{Clayton2022}.

Next, although computational models are useful --- for example, when evaluating large amounts of data --- there is always an element of subjectivity involved in interpreting them, particularly with relation to complex issues like style. This is in part due to the way in which several models can learn to rely on different features despite fitting the underlying data equally well (\cite{Fisher2019, Rudin2024}, see also the discussion in Section~\ref{sec:rsi_feature_importance_by_domain}), but also due to the uncertainty involved in suggesting that they ever represent an underlying perceptual or sensory experience. In order to truly claim (for instance) that harmony is more or less important in distinguishing jazz musicians than melody (see, here, Figure~\ref{fig:rsi_sm_feature_importance_by_handcrafted_model_type}), we would need to conduct a perceptual study involving a large number of participants familiar with jazz improvisation. While we took steps towards this in Section~\ref{sec:mp_listener_evaluations}, our study here was admittedly simplistic in order to ensure that the largest number of individuals could participate. A more advanced study would need to consider how best to present the question of identifying distinct jazz improvisation styles to non-experts, perhaps using the method proposed by \citet{Gillick2010}; however, as we found in Section \ref{sec:gen_subjective_evaluation}, this is not a straightforward question. Relating human judgements to those made by our models provides an exciting opportunity for further research that engages not only with \GLS{MIR} but also music psychology and cognition. 

We also did not conduct a formal user evaluation of the interactive web applications described in Chapters~\ref{chap:jtd_tismir}--\ref{chap:rhythm_rsos}. Although we based the format of many of these applications on prior research outputs \citep[e.g., those described in][]{Frieler2018}, it is admittedly unclear at this point exactly how jazz students would prefer to see this material presented. Designing effective platforms for music education is an under-explored area in \GLS{MIR} \citep[see][for a discussion]{Dittmar2012} and this work could provide materials for an interesting future case study. This would also provide an opportunity for this work to engage more formally with music education research. Furthermore, we could expand this user study also to consider the generative model introduced in Chapter~\ref{chap:condition_gen}.

Throughout this research, we addressed performance style in jazz in isolation from other genres, such as Western classical and popular music. However, it is worth noting that several of the performers we have included in our database (for instance, Chick Corea and Keith Jarrett) have also enjoyed successful careers performing non-jazz music. This, presumably, involves many similar stylistic features (e.g., rhythm and dynamics patterns) that we considered here with relation to jazz \citep[see][]{Zhang2023-Horowitz}. As a result, the models described in this work --- particularly the multi-input model discussed in Section~\ref{sec:rsi_factorised_inputs_approach} --- would also be relevant for this genre, too. A few cross-genre studies have been conducted into the importance of different musical features in automatic classifications of jazz and classical music as a whole \citep[e.g.,][]{Harrison2018-harmony, Broze2013}, but none have yet considered individual performers who play both types of music (for instance: does Keith Jarrett's idiosyncratic use of rhythm in jazz cross over into his jazz playing?). This would be an interesting area for future modelling research, and studying these recordings could perhaps inspire related work in the area of music style transfer \citep[e.g.,][]{George2019}.

Our corpus analyses described in Chapters~\ref{chap:xai_rsi} and~\ref{chap:rhythm_rsos} were limited to two specific musical datasets of jazz recordings, \GLS{JTD} and \GLS{PiJAMA}. It is possible that the conclusions in this research are sensitive either to the sampling methods used in constructing both datasets, or the methods they used for extracting data from an audio recording. For instance, both \GLS{JTD} and \GLS{PiJAMA} used the transcription model described by \citet{Kong2021} to transcribe MIDI from piano audio. This method was state-of-the-art at the time both datasets were compiled. However, recent models have surpassed it in terms of performance, either by adopting alternative architectures \citep{Yan2024} or by retraining from scratch with data augmentation \citep{Edwards2024}. The best way to cement the conclusions drawn in this work would be to either update the annotations in both datasets with these newer methods, or to repeat them using alternative datasets of annotated jazz improvisations \citep[e.g.,][]{Pfleiderer2017, Foster2021}. In addition, both datasets are highly biased towards male jazz musicians active in the United States around the middle of the 20th century, and thus the recordings contained within them may not fully represent the diversity of jazz improvisation styles.

The software described in this thesis was mostly written in the Python programming language, with the machine-learning models implemented using the \texttt{PyTorch} \citep{Paszke2019} and \texttt{scikit-learn} \citep{Pedregosa2011} frameworks. These are all widely used by the scientific community. As mentioned above, we have recently integrated \GLS{JTD} with the \texttt{mirdata} package \citep[see Section~\ref{sec:jtd_subsequent_developments}]{Bittner2019}, which should make working with this dataset straightforward for the end user as well. In the future, we could also port it to the \texttt{datasets} package (available as part of the \texttt{huggingface} ecosystem\footnote{\url{https://huggingface.co/}}), which would help make \GLS{JTD} accessible to the largest possible number of users.