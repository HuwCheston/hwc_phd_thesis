\chapter{Discussion}\label{chap:discussion}

The research outlined in this thesis was driven by two central motivations, discussed in section \ref{sec:intro_motivations}. The first was to enrich our understanding of the distinct styles of jazz improvisation that have emerged over the past hundred years by applying computational modelling techniques to large databases of annotated recordings. The second was to develop resources (including pre-trained models, datasets, software, and online web applications) that subsequent scientific research could take advantage of. 

Much of this work relied on a collection of annotated recordings by jazz rhythm sections (the Jazz Trio Database or \GLS{JTD}) that is described in Chapter \ref{chap:jtd_tismir}. \GLS{JTD} contains 44.5 hours of jazz piano solos annotated by an automated pipeline, with data provided for all three performers (i.e., 133.5 hours of total annotations). This pipeline consisted of two stages whereby the raw audio is first ``demixed'' using music source separation models to create isolated piano, bass, and drums signals: each signal is subsequently annotated using automatic transcription algorithms optimized for that instrument. The annotations provided include onset times for each performer, MIDI transcriptions for the piano soloist, and beat and downbeat transcriptions for the audio mixture. Onset annotations showed broad agreement to human annotations, with a mean $F$-score of 0.94. Recordings suitable for inclusion in \GLS{JTD} were identified by scraping user-based listening data and cross-referencing these with existing jazz discographies. Comparison between \GLS{JTD} and other datasets of jazz performances revealed broad similarities across a range of objective metrics. This database serves as a valuable resource for researchers investigating improvised music and can be utilised for a wide range of \GLS{MIR} tasks, including artist identification, expressive performance modelling, and symbolic music generation. 

A recurring theme of the following chapters was to respond to a statement made by \citet{Berendt1976} that
\begin{quote}
    \centering
    there doesn't [yet] exist ... any form of notation, graphic representation, and computer analysis capable of satisfactorily registering the subtlety of ... [the] specific distinctions between groups and individuals \citep[p. 192]{Berendt1976}.
\end{quote}
in jazz. To do so, Chapters \ref{chap:xai_rsi}--\ref{chap:mp_network} addressed three core phenomena in jazz improvisation using computational methods: musical style, rhythm, and ensemble interaction.

Chapter \ref{chap:xai_rsi} considered improvisation style --- Berendt's ``specific distinctions between individuals'' --- encompassing a performer's use of particular melodic lines and rhythmic phrasing, amongst other factors. Understanding exactly which of these elements best reflect and define their style can be difficult, however, and may act as a barrier to learning and appreciating this music. We used machine learning to address this issue by training supervised-learning models to identify twenty pianists from recordings contained in \GLS{JTD} and \GLS{PiJAMA} using different forms of input. We focussed on three types of models, trained respectively on (1) ``handcrafted'' feature sets, (2) representations learned directly from an input, and (3) a novel ``factorised'' input structure. The ``handcrafted'' models took as an input the counts of melodic patterns and chord voicings and facilitated straightforward interpretation, at the expense of lower prediction accuracy. The ``representation learning'' models improved substantially in terms of accuracy, but existing post-hoc interpretability techniques \citep[e.g.,][]{Ribeiro2016} failed to yield clear insights into jazz improvisation style. This chapter culminated with the introduction of a deep neural network architecture (inspired by mixture-of-experts modelling) that achieved state-of-the-art prediction accuracy (91\% accuracy) with a factorised structure that allowed its predictions to be explained in terms of four fundamental musical domains (melody, harmony, rhythm, and dynamics). Our analyses of these models highlighted the relative importance of each domain in distinguishing jazz pianists and uncovered a variety of melodic and harmonic patterns associated with each performer. 

Chapter \ref{chap:rhythm_rsos} considered rhythm --- in particular, the distinctiveness of the rhythmic style of individual performers, which was shown to be important by our factorised model from Chapter \ref{chap:xai_rsi} but not considered in any of the subsequent analyses included within this chapter. We trained a supervised classification model to identify the performer in a recording using a targeted set of nineteen rhythmic features derived from ethnographic and pedagogical literature on jazz. Despite the limited size of the feature set used to train this model compared with those in the preceding chapter, it nonetheless achieved 59\% accuracy when identifying ten performers within the \GLS{JTD-300} subset, approximately six times better than chance. The most important features used by this model related to a performer's ``feel'' (ensemble synchronization) and ``complexity'' (information density), with lesser importance given to those relating to ``interaction'' (modelled sensorimotor coupling). Interpreting the distribution of these features also revealed insights into the style of each performer that was typically consistent with the critical and musicological writing on their music. Further analysis of our model revealed two clusters of performers, with those in the same cluster sharing similar rhythmic traits. Additionally, the rhythmic style of each musician changed relatively little over the duration of their career, and there were no apparent differences in rhythmic feature usage across recordings made in different historical eras.

Chapter \ref{chap:mp_network} considered ensemble coordination --- Berendt's `` specific distinctions between groups'' --- using many of the same features that we first defined in Chapters \ref{chap:jtd_tismir} and \ref{chap:rhythm_rsos}. Group jazz performance depends on the temporal coordination of action in order to be effective. However, studying the means by which this coordination is achieved in real-time performances may not yield meaningful insight into the differences between ensembles, especially when musicians optimize their timing simply to reduce the total asynchrony \citep[see section \ref{sec:rsos_feature_importance}][]{Jacoby2021}. Instead, we use temporal latency (of the kind introduced on networking platforms such as Zoom) to disrupt the coordination process by introducing a delay between when a musical sound is produced and when it is received. This can result in significant deteriorations in synchrony and stability between performers. Through a study of five professional rhythm sections, we found two strategies that involve: (1) one musician being led by the other, tracking the timings of the leader's performance, and (2) both musicians accommodating to each other, mutually adapting their timing. During networked performance, these two strategies favoured different sides of a trade-off between (respectively) tempo synchrony and stability; while, in the absence of delay, both achieve similar outcomes. Further analysing how the musicians in each ensemble described their own performance led to insights into their aesthetic priorities and improvisation style that would be difficult to acquire simply from studying and modelling commercial recordings (as in Chapters \ref{chap:jtd_tismir}--\ref{chap:rhythm_rsos}).

One conclusion from these studies is clear: computational modelling \textit{can} register the subtlety of the specific distinctions between groups and individual in jazz, at least in part. One way that it can do so is by providing a means to objectively test terminology that appears frequently in the critical writing on jazz, such as ``groove'' and ``interaction''. An example of this in practice can be found in the discussions of musical style contained in Chapters \ref{chap:xai_rsi} and \ref{chap:rhythm_rsos}. What makes a jazz musician sound like ``themselves'' might be self-evident to a knowledgeable listener, but difficult to isolate in specific examples or disentangle from contextual factors (e.g., standard compositions that they are best known for, acoustic qualities of recordings from different historical periods). Good models can process large datasets and generate interpretable insights from them: for instance, in Chapter \ref{chap:rhythm_rsos} we were able to demonstrate the relative importance of rhythmic qualities like ``swing'' and ``feel'' to the styles of different pianists. Such insights not only help clarify long-standing debates in jazz scholarship, but also provide tools for exploring stylistic evolution, influence, and innovation within the genre. 

A second conclusion here is how computational modelling can validate and extend accounts given by both musicologists and performers. In every chapter, we demonstrated how our supervised-learning models can be interpreted in ways that appear to reinforce claims made by the performers themselves. In one sense, this is obvious: our models could simply be said to show that musicians ``do what they say they do''. But, in another, it is highly interesting. Considering several of the comments by the musicians featured in Chapter \ref{chap:mp_network} (see also Appendix \ref{chap:mp_appendix}) --- and also in interviews with many of the pianists with performances in \GLS{JTD} \citep[e.g.,][]{Sidran1992, Berliner1994, Monson1996} --- it is clear that musicians are not always able (or willing) to describe their own performance style. Computational techniques can help capture the complexities of these styles: in this sense, they do not merely confirm existing views on jazz improvisation, but can also reveal deeper structures of musical thought and practice that might otherwise have remained inaccessible to researchers. 

We have supported this work with a range of open-source software packages, datasets, and web applications. \GLS{JTD} is integrated with the widely used \texttt{mirdata} library \citep{Bittner2019} to facilitate straightforward and reproducible experimentation in the Python programming language with a minimum of boilerplate code. We have developed several web applications relating to \GLS{JTD}: the first allows a variety of features extracted from its recordings to be explored in a browser,\footnote{\url{https://huwcheston.github.io/Jazz-Trio-Database/resources/data-explorer.html}} and the second provides an interactive graph of the playing networks between the different musicians (piano, bass, and drums) with recordings in \GLS{JTD-300}.\footnote{\url{https://huwcheston.github.io/Jazz-Trio-Database/resources/trio-network-search.html}} The models introduced in Chapter \ref{chap:xai_rsi} are accompanied by an interactive web application that allows users to listen to examples of the melodic patterns associated with performers in \GLS{JTD} (section \ref{sec:rsi_handcrafted_feature_extraction}), as well as view the overall distinctive components of their improvisation style (section \ref{sec:rsi_factorised_domain_importance}).\footnote{\url{https://huwcheston.github.io/ImprovID-app/index.html}} The model presented in Chapter \ref{chap:rhythm_rsos} is accompanied by another web application that plots the predictions of the model on an interactive map, such that recordings in \GLS{JTD} with inherent rhythmic similarities are placed closer together.\footnote{\url{https://huwcheston.github.io/Jazz-Trio-Database/_static/prediction-app.html}} Finally, Chapter 5 resulted in the development of a software platform (described in section \ref{sec:mp_testbed}) capable of introducing controlled, deterministic manipulations (including --- but not limited to --- network delay) into real-time music performances.\footnote{\url{https://github.com/HuwCheston/AV-Manip}} This research also involved compiling a dataset of MIDI recordings with accompanying video that could be used in downstream \GLS{MIR} tasks such as audio-to-video alignment, and a dataset of user evaluations that could be used in cognitive modelling.\footnote{\url{https://doi.org/10.5281/zenodo.7773824}} We hope that these packages and applications will prove useful for subsequent computational research into jazz improvisation. 


\section{Limitations and future directions}\label{sec:discussion_limitations_and_future_directions}

We can foresee several limitations of the work described in this thesis. For one, our research was restricted to modelling jazz improvisation in the symbolic domain. The motivation behind this restriction was that symbolic representations such as MIDI ``piano rolls'' are widely used by musicians and thus lend themselves to meaningful interpretations, over and above auditory representations such as spectrograms, which may not be as straightforward to understand \citep{Foscarin2022}. However, expressive jazz improvisation engages with a greater number of parameters than can be feasibly be captured by a symbolic representation. For instance, aspects of musical tone and timbre are involved in expressive performances on many instruments other than the piano \citep{Abeser2015-scoreinformed, Weis2018}, but cannot efficiently be captured in the form of a piano roll. These factors would need to be considered if the methodology developed here were to be applied to instruments other than the piano, likely through the use of an audio-based (as opposed to symbolic) representation \citep[here, see particularly][]{Ramirez2010}.

Other expressive parameters that were not considered in this work instead relate to the visual dimension. This is important not only for allowing musicians to coordinate effectively with each other \citep[][see also several comments in Appendix \ref{chap:mp_appendix}]{Chang2017, Moran2015, Doffman2014}, but also (through the use of particularly idiosyncratic bodily movements, for instance) becomes a key part of the musical style of several jazz musicians considered here, such as Keith Jarrett \citep{Elsdon2013}. Multi-modal research in \GLS{MIR} has seen some recent attention \citep[for a review, see][]{Simonetta2019}, although the majority of this work has considered textual information alongside audio, rather than visual representations. As part of the research described in Chapter \ref{chap:mp_network}, we gathered over four hours of video recordings of professional jazz musicians performing that are time-aligned with corresponding MIDI transcriptions. A closer analysis of this data could prove useful, for instance, in developing audio-to-image alignment models, which is an under-explored task in \GLS{MIR}, or for better understanding the types of visual coordination involved in musical improvisation \citep{Chang2017, Moran2015}.

Related to the above point is how we considered improvisation style using almost entirely computational and statistical methods. Although computational models are useful, for example, when evaluating large amounts of data, there is always an element of subjectivity involved in interpreting them. This is in part due to the way in which several models can learn to rely on different features despite fitting the underlying data equally well \citep[see also the discussion in section \ref{sec:rsi_feature_importance_by_domain}]{Fisher2019, Rudin2024}, but also due to the uncertainty involved in ever suggesting that they represent an underlying perceptual or sensory experience. In order to truly claim (for instance) that harmony is more or less important in distinguishing jazz musicians than melody (see, here, Figures \ref{fig:rsi_sm_feature_importance_by_handcrafted_model_type}), we would need to conduct a perceptual study involving a large number of participants familiar with jazz improvisation. While we took steps towards this in section \ref{sec:mp_listener_evaluations}, our study here was admittedly simplistic in order to ensure that the largest number of individuals could participate. A more advanced study would need to consider how best to present the question of identifying distinct jazz improvisation styles to non-experts \citep[perhaps using the method proposed by][]{Wong2020}. Relating human judgements to those made by our models provides an exciting opportunity for further research that engages not only with \GLS{MIR} but also music psychology and cognition. 

Additionally, this work could further engage with music pedagogy by conducting formal user studies of the interactive web applications described in Chapters \ref{chap:jtd_tismir}--\ref{chap:rhythm_rsos}. Although we based the format of many of these applications on prior research outputs \citep[e.g., those described in][]{Frieler2018}, it is admittedly unclear at this point exactly how jazz students would prefer to see this material presented. Designing effective platforms for music education is an under-explored area in \GLS{MIR} \citep[see][for a discussion]{Dittmar2012} and this work could provide materials for an interesting future case study.

Throughout this research we addressed jazz improvisation in isolation from other genres, such as Western classical and popular music. However, it is worth noting that several of the performers we have included in our database (for instance, Chick Corea and Keith Jarrett) have enjoyed successful careers performing non-jazz music. This, presumably, involves many similar stylistic features (e.g., rhythm and dynamics patterns) that we considered with relation to jazz, and hence the models described in this work (particularly the factorised model discussed in section \ref{sec:rsi_factorised_inputs_approach}) would also be relevant here. A few cross-genre studies have been conducted into the importance of different musical features in automatic classifications of jazz and classical music as a whole \citep[e.g.,][]{Harrison2018-harmony, Broze2013}, but none have yet considered individual performers who play both types of music. This would be an interesting area for future expansion of our database, and could perhaps inspire related work in the area of music style transfer \citep[e.g.,][]{George2019}.

Our corpus analyses described in Chapters \ref{chap:xai_rsi} and \ref{chap:rhythm_rsos} were limited to two specific musical corpora of jazz recordings, \GLS{JTD} and \GLS{PiJAMA}. It is possible that the conclusions in this research are sensitive either to the sampling methods used in constructing these corpora, or the methods used for extracting data from an audio recording of a jazz performance. For instance, both datasets used the transcription model described by \citet{Kong2021} to generate MIDI from audio. At the time both datasets were compiled this was the state-of-the-art method; however, recent models have surpassed it in terms of performance, both by adopting alternative architectures \citep{Yan2024} and from retraining from scratch with data augmentation \citep{Edwards2024}. The best way to cement the conclusions drawn in this work would be to either update the annotations in both datasets with these newer methods, or to repeat them using alternative datasets of annotated jazz improvisations \citep[e.g.,][]{Pfleiderer2017, Foster2021}.

The software and models described in this thesis were mostly written in the programming language Python, with the web applications powered by JavaScript. We have recently integrated \GLS{JTD} with the \texttt{mirdata} package \citep[see section \ref{sec:jtd_subsequent_developments}]{Bittner2019}, which should make working with this dataset in Python straightforward for the end user. However, it would also be worthwhile to port some of the models outlined here to ecosystems such as \texttt{huggingface},\footnote{\url{https://huggingface.co/}} which would facilitate their wider adoption by the machine learning community.