\chapter{Discussion}\label{chap:discussion}

The research outlined in this thesis was driven by two central motivations, discussed in section \ref{sec:intro_motivations}. The first was to understand individual differences in jazz improvisation styles by applying supervised learning techniques and computational modelling to large databases of symbolic music representations. The second was to develop resources (including pre-trained models, datasets, software, and online web applications) that can be taken advantage of in future research. 

Much of this work relied on a collection of annotated audio recordings by jazz rhythm sections (the Jazz Trio Database or \GLS{JTD}) that is described in Chapter \ref{chap:jtd_tismir}. \GLS{JTD} contains 44.5 hours of jazz piano solos (1,294 recordings) by 34 different pianists annotated by an automated pipeline, with data provided for all three performers. This pipeline consisted of two stages whereby the raw audio is first ``demixed'' using music source separation models to create isolated piano, bass, and drums signals: each signal is subsequently annotated using automatic transcription algorithms optimized for that instrument. The annotations include onset times for each performer, MIDI transcriptions for the piano soloist, and beat and downbeat transcriptions for the audio mixture. \GLS{JTD} fulfils a niche lacking in existing datasets insofar that, by including annotations for multiple performers, it enables interesting group-level musical features to be analysed and modelled. We anticipate that it will prove useful for a wide range of downstream \GLS{MIR} tasks, including artist identification, expressive performance modelling, and symbolic music generation --- potentially applied to an entire jazz ensemble. 

A recurring theme of the following chapters was to respond to a statement that
\begin{quote}
    \centering
    there doesn't exist ... any form of notation, graphic representation, and computer analysis capable of satisfactorily registering the subtlety of [the musical processes in jazz, that] differ not just stylistically but also in terms of specific distinctions between groups and individuals \citep[p. 192]{Berendt1976}.
\end{quote} More specifically, we wanted to consider whether recent advances in machine learning and audio signal processing now enable us to develop computational tools capable of capturing these subtleties. Across Chapters \ref{chap:xai_rsi}--\ref{chap:mp_network} we described a variety of tools, addressing three core phenomena in jazz improvisation: musical style, rhythm, and ensemble interaction.

Chapter \ref{chap:xai_rsi} considered improvisation style --- the ``specific distinctions between individuals'' --- encompassing a performer's use of particular melodic lines and rhythmic phrasing, amongst other factors. Understanding exactly which of these elements best reflect and define their style can be difficult, however, and may act as a barrier to learning and appreciating this music. We used machine learning to address this issue by training supervised-learning models to identify twenty pianists from recordings contained in \GLS{JTD} and \GLS{PiJAMA} using different forms of input. We focussed on three types of models, trained respectively on (1) ``handcrafted'' feature sets, (2) representations learned directly from an input, and (3) a novel ``factorised'' input structure. This chapter culminated with the introduction of a deep neural network architecture that achieved near state-of-the-art prediction accuracy with a multi-input structure that allowed its predictions to be explained in terms of four fundamental musical domains (melody, harmony, rhythm, and dynamics). Our analyses of these models highlighted the relative importance of each domain in distinguishing jazz pianists and uncovered a variety of melodic and harmonic patterns associated with each performer.

Chapter \ref{chap:rhythm_rsos} considered rhythm --- in particular, the distinctiveness of the rhythmic style of individual performers, which was shown to be highly important to our multi-input model, but not considered in detail in the previous chapter. We trained a separate supervised-learning model to identify the performer in a recording using a targeted set of nineteen rhythmic features derived from ethnographic and pedagogical literature on jazz. Despite the limited size of the feature set used to train this model, it nonetheless achieved 59\% accuracy when identifying ten performers within the \GLS{JTD-300} subset. The most important features used by this model related to a performer's ``feel'' (ensemble synchronization) and ``complexity'' (information density), with lesser importance given to those relating to ``interaction'' (modelled sensorimotor coupling). Interpreting the distribution of these features also revealed insights into the style of each performer that were generally consistent with the critical writing on their music. The rhythmic style of each musician changed relatively little over the duration of their career, and there were no apparent differences in rhythmic feature usage across recordings made in different historical eras.

Chapter \ref{chap:mp_network} considered ensemble coordination --- the ``specific distinctions between groups''. Group jazz performance depends on the temporal coordination of action in order to be effective. However, studying the means by which this coordination is achieved in real-time performances may not yield meaningful insight into the differences between ensembles, especially when musicians optimise their timing simply to reduce the total asynchrony \citep[see section \ref{sec:rsos_feature_importance}][]{Jacoby2021}. Instead, we use temporal latency to disrupt coordination by introducing a delay between when a musical sound is produced and when it is received. In a study of ten professional jazz duos, we found two strategies that involve: (1) one musician being led by the other, tracking the timings of the leader's performance, and (2) both musicians accommodating to each other, mutually adapting their timing. During networked performance, these two strategies favoured different sides of a trade-off between tempo synchrony and stability; in the absence of delay, both achieved similar outcomes. Further analysing how the musicians in each ensemble described their own performances led to insights into their improvisation style that would be difficult to acquire simply by modelling commercial recordings.

Given the work conducted in the previous chapters, one conclusion is clear: computational modelling \textit{does} now appear to offer one way of studying the specific distinctions between groups and individuals in jazz. One way that it can do so is by providing a means to objectively test terms and concepts that appear in musicological literature. For example: what makes a jazz musician sound like ``themselves'' might be self-evident to a knowledgeable listener, but equally it can also be difficult to isolate in specific examples or disentangle from contextual factors (e.g., standard compositions that they are best known for, acoustic qualities of recordings from different historical periods). We have shown how advances in audio signal processing enable complex features to be extracted from large datasets in ways that would have previously required painstaking manual analysis. By training supervised-learning models on these features, we can apply techniques from explainable artificial intelligence --- as well as bespoke methods designed specifically for jazz (section \ref{sec:rsi_factorised_architecture}) --- to unpick the importance of abstract qualities like ``swing'' and ``feel'' to the style of particular jazz performers and groups. Such insights not only help clarify long-standing debates in jazz scholarship, but also provide tools for exploring stylistic evolution, influence, and innovation within the genre.

A second conclusion here is how the insights obtained from good models can validate and extend accounts given by performers themselves. In every chapter, we demonstrated how our supervised-learning models can be interpreted in ways that appear to reinforce claims made by particular musicians or groups. One could argue that this is obvious: our models simply show that musicians ``do what they say they do''. But, from another perspective, this is highly interesting. Considering several of the comments by the musicians featured in Chapter \ref{chap:mp_network} (see also Appendix \ref{chap:mp_appendix}) --- and also in interviews with many of the pianists with performances in \GLS{JTD} \citep[e.g., those in][]{Sidran1992, Berliner1994, Monson1996} --- it is clear that jazz musicians are not always able (or willing) to describe their own performance style. We have shown how computational techniques can help capture the complexities of these styles. In this sense, a good model does not merely confirm existing views on jazz improvisation, but can also reveal deeper structures of musical thought and practice that might otherwise have remained inaccessible to researchers.

A second motivation for this work was to support future computational analysis of jazz improvisation with a range of open-source software packages, datasets, and web applications. Perhaps our most substantive contribution here is \GLS{JTD} itself, which is now a part of the widely used \texttt{mirdata} library \citep{Bittner2019} --- enabling straightforward and reproducible experimentation by members of the \GLS{MIR} community. In addition, we have also developed several web applications relating to \GLS{JTD}. The first allows a variety of features extracted from its recordings to be explored in a browser,\footnote{\url{https://huwcheston.github.io/Jazz-Trio-Database/resources/data-explorer.html}} and the second provides an interactive graph of the playing networks between the different musicians (piano, bass, and drums) with recordings in \GLS{JTD-300}.\footnote{\url{https://huwcheston.github.io/Jazz-Trio-Database/resources/trio-network-search.html}} The models introduced in Chapter \ref{chap:xai_rsi} are accompanied by an interactive web application that allows users to listen to examples of the melodic patterns associated with performers in \GLS{JTD} (section \ref{sec:rsi_handcrafted_feature_extraction}), as well as view the overall distinctive components of their improvisation style (section \ref{sec:rsi_factorised_domain_importance}).\footnote{\url{https://huwcheston.github.io/ImprovID-app/index.html}} The model presented in Chapter \ref{chap:rhythm_rsos} is accompanied by another web application that plots the predictions of the model on an interactive map, such that recordings in \GLS{JTD} with inherent rhythmic similarities are placed closer together.\footnote{\url{https://huwcheston.github.io/Jazz-Trio-Database/_static/prediction-app.html}} 

Finally, as part of the work conducted in Chapter \ref{chap:mp_network}, we have developed a software platform (described in section \ref{sec:mp_testbed}) capable of introducing controlled, deterministic manipulations (including --- but not limited to --- network delay) into real-time music performances. We also compiled an additional dataset of four hours of MIDI, audio, and video recordings by professional jazz pianists and drummers that could be used to train downstream \GLS{MIR} models in tasks such as audio-to-video alignment, as well as a dataset of user evaluations that could be used either in cognitive modelling or to fine-tune music language models to align with human preferences.\footnote{\url{https://doi.org/10.5281/zenodo.7773824}} We hope that these packages, web applications, and datasets will prove useful to members of the community hoping to conduct subsequent computational research into jazz improvisation. 

\section{Limitations and future directions}\label{sec:discussion_limitations_and_future_directions}

We can foresee several limitations of the work described in this thesis. Firstly, our research was restricted to modelling jazz improvisation in the symbolic domain. Our reasoning was that symbolic representations such as MIDI ``piano rolls'' are widely used by musicians and thus lend themselves to meaningful interpretations --- over and above auditory representations such as spectrograms, which may not be as straightforward to understand \citep{Foscarin2022}. However, expressive jazz improvisation engages with a greater number of parameters than can be feasibly be captured by a symbolic representation. For instance, aspects of musical tone and timbre are involved in expressive performances on many instruments other than the piano \citep{Abeser2015-scoreinformed, Weis2018}, and these features cannot efficiently be captured in a symbolic representation. These factors would need to be considered if the methodology developed here were to be applied to a wider range of instruments, likely through the use of an audio-based (as opposed to symbolic) representation \citep[here, see particularly][]{Ramirez2010}.

We also did not consider the visual dimension in our work. This is important not only for allowing musicians to coordinate effectively with each other (\cite{Chang2017, Moran2015, Doffman2014}; see also several comments in Appendix~\ref{chap:mp_appendix}), but also --- through the use of particularly idiosyncratic bodily movements, for instance --- becomes a key part of the musical style of several jazz musicians considered here, such as Keith Jarrett \citep{Elsdon2013}. Ideally, the phase correction models described in Chapter \ref{chap:mp_network} would be augmented with equivalent analysis of the video data that we gathered at the same time (see section \ref{sec:mp_testbed}). This would allow for a more substantive analysis of lead-lag phenomena along the lines of \citet{Chang2017} and \citet{Moran2015}. Incorporating video alongside audio can also prove useful in several \GLS{MIR} tasks related to performer identification, such as genre and style classification \citep{Clayton2022}.

Next, although computational models are useful --- for example, when evaluating large amounts of data --- there is always an element of subjectivity involved in interpreting them. This is in part due to the way in which several models can learn to rely on different features despite fitting the underlying data equally well (\cite{Fisher2019, Rudin2024}, see also the discussion in section~\ref{sec:rsi_feature_importance_by_domain}), but also due to the uncertainty involved in suggesting that they ever represent an underlying perceptual or sensory experience. In order to truly claim (for instance) that harmony is more or less important in distinguishing jazz musicians than melody (see, here, Figures \ref{fig:rsi_sm_feature_importance_by_handcrafted_model_type}), we would need to conduct a perceptual study involving a large number of participants familiar with jazz improvisation. While we took steps towards this in section \ref{sec:mp_listener_evaluations}, our study here was admittedly simplistic in order to ensure that the largest number of individuals could participate. A more advanced study would need to consider how best to present the question of identifying distinct jazz improvisation styles to non-experts \citep[perhaps using the method proposed by][]{Wong2020}. Relating human judgements to those made by our models provides an exciting opportunity for further research that engages not only with \GLS{MIR} but also music psychology and cognition. 

We also did not conduct a formal user evaluation of the interactive web applications described in Chapters \ref{chap:jtd_tismir}--\ref{chap:rhythm_rsos}. Although we based the format of many of these applications on prior research outputs \citep[e.g., those described in][]{Frieler2018}, it is admittedly unclear at this point exactly how jazz students would prefer to see this material presented. Designing effective platforms for music education is an under-explored area in \GLS{MIR} \citep[see][for a discussion]{Dittmar2012} and this work could provide materials for an interesting future case study. This would also provide an opportunity for this work to engage more formally with music education research.

Throughout this research, we addressed jazz improvisation in isolation from other genres, such as Western classical and popular music. However, it is worth noting that several of the performers we have included in our database (for instance, Chick Corea and Keith Jarrett) have also enjoyed successful careers performing non-jazz music. This, presumably, involves many similar stylistic features (e.g., rhythm and dynamics patterns) that we considered here with relation to jazz \citep[see][]{Zhang2023-Horowitz}. As a result, the models described in this work --- particularly the multi-input model discussed in section \ref{sec:rsi_factorised_inputs_approach} --- would also be relevant for this genre, too. A few cross-genre studies have been conducted into the importance of different musical features in automatic classifications of jazz and classical music as a whole \citep[e.g.,][]{Harrison2018-harmony, Broze2013}, but none have yet considered individual performers who play both types of music (for instance: does Keith Jarrett's idiosyncratic use of rhythm in jazz cross over into his jazz playing?). This would be an interesting area for future modelling research, and studying these recordings could perhaps inspire related work in the area of music style transfer \citep[e.g.,][]{George2019}.

Our corpus analyses described in Chapters \ref{chap:xai_rsi} and \ref{chap:rhythm_rsos} were limited to two specific musical datasets of jazz recordings, \GLS{JTD} and \GLS{PiJAMA}. It is possible that the conclusions in this research are sensitive either to the sampling methods used in constructing both datasets, or the methods they used for extracting data from an audio recording. For instance, both \GLS{JTD} and \GLS{PiJAMA} used the transcription model described by \citet{Kong2021} to transcribe MIDI from piano audio. This method was state-of-the-art at the time both datasets were compiled; however, recent models have surpassed it in terms of performance, either by adopting alternative architectures \citep{Yan2024} or by retraining from scratch with data augmentation \citep{Edwards2024}. The best way to cement the conclusions drawn in this work would be to either update the annotations in both datasets with these newer methods, or to repeat them using alternative datasets of annotated jazz improvisations \citep[e.g.,][]{Pfleiderer2017, Foster2021}. In addition, both datasets are highly biased towards male jazz musicians active in the United States around the middle of the twentieth century, and thus the recordings contained within them may not fully represent the diversity of jazz improvisation styles.

The software described in this thesis was mostly written in the Python programming language, with the machine-learning models implemented using the \texttt{PyTorch} \citep{Paszke2019} and \texttt{scikit-learn} \citep{Pedregosa2011} frameworks. These are all widely used by the scientific community. As mentioned above, we have recently integrated \GLS{JTD} with the \texttt{mirdata} package \citep[see section \ref{sec:jtd_subsequent_developments}]{Bittner2019}, which should make working with this dataset straightforward for the end user as well. In the future, we could also port it to the \texttt{datasets} package (available as part of the \texttt{huggingface} ecosystem\footnote{\url{https://huggingface.co/}}), which would help make \GLS{JTD} accessible to the largest possible number of users.