\chapter{Introducing the Jazz Trio Database}\label{chap:jtd_tismir}

\section{Introduction}

Given its lack of notated scores and the freedoms afforded to its performers, improvised jazz is a musical genre that has often resisted computational analysis. Recent advances in automatic music transcription, however, have enabled the creation of several large-scale databases of annotated jazz recordings \citep[e.g.,][]{Edwards2023, Foster2021}. This allows researchers to scale up the analysis of jazz improvisation to a degree that has not previously been possible.

Previous research has tended to focus on individual soloists and the annotation of accompanying musicians has tended to be incomplete or partial (only some members of the ensemble, for instance). Our goal in this project is to develop a database that includes annotations for every musician in an improvising jazz ensemble. We focus primarily on timing, as this enables the analysis of interesting group-level musical features, such as interaction and synchronisation. However, we also wish to include automatically extracted pitch and velocity data for instruments where this has seen some previous work \citep[e.g., piano: see][]{Kong2021}.
		
To extract data from a mixed ensemble recording, we leverage recent developments in audio source separation and timing annotation. Using deep learning, it has become possible to separate isolated sources from an audio mixture with massively increased fidelity compared to earlier approaches. The quality of separation still depends on the instrument, however, with vocals, bass, piano, and drums separation having seen the majority of work, while the separation of brass and stringed instruments remains at an earlier stage.
		
To this end, in this chapter we introduce the Jazz Trio Database (\GLS{JTD}), a database of machine-annotated recordings of piano soloists improvising with bass and drums accompaniment. These instruments form the standard ``rhythm section'' that has accompanied soloists in jazz since the 1940s, and can be thought of as providing the crucial contexts within which their improvisations are articulated and developed. To this extent, results obtained from our database could readily be generalised beyond the piano trio.

We identify recordings for inclusion in \GLS{JTD} by scraping user-based listening and discographic data and pulling audio from YouTube. We then use source separation models to extract isolated audio from each instrument, and finally apply a range of automatic transcription algorithms to automatically generate annotations from each source.
		
In this chapter, we describe several related datasets, discuss the curation of \GLS{JTD}, outline the data extraction pipeline we developed, and explore \GLS{JTD} by conducting several analyses. We envisage that \GLS{JTD} will be useful in a variety of \GLS{MIR} tasks, such as artist identification, symbolic music generation, and expressive performance modelling.

\section{Related Work}

\begin{sidewaystable}[ph!]
\centering
\caption[Comparison of existing datasets for each instrument in the jazz piano trio.]{Comparison of existing datasets for each instrument in the jazz piano trio. *Note that RWC-Jazz has not been made available free and open source, meaning that it is not possible to provide full detail here.}
\label{tab:jtd_existing_datasets}
\begin{tabular}{l c c c c l l}
\toprule
\textbf{Instrument} & \textbf{Name} & \textbf{Method} & \textbf{Tracks} & \textbf{Duration (s)} & \textbf{Annotations} & \textbf{Metadata} \\
\midrule
\textbf{Piano} & \GLS{WJD} & Manual & 6 & 582 & 3,149 onsets & Beat, chord, section \\
 & \GLS{PiJAMA} & Automatic & 2,777 & 804,960 & 7,108,460 MIDI notes & N/A \\
 & RWC-Jazz & Manual & 5 & 1,672 & N/A* & Beat, section \\
 & \textbf{\GLS{JTD} (ours)} & Automatic & 1,294 & 159,668 & 866,116 onsets, 2,174,833 MIDI notes & Beat \\ \midrule
\textbf{Bass} & \GLS{WJD} & Automatic & 426 & 49,010 & 5,000 beat-wise pitches & Beat, chord, section \\
 & FiloBass & Automatic + manual & 48 & 17,880 & 53,646 MIDI notes & Downbeat, chord \\
 & RWC-Jazz & Manual & 5 & 1,672 & N/A* & Beat, section \\
 & \textbf{\GLS{JTD} (ours)} & Automatic & 1,294 & 159,668 & 543,693 onsets & Beat \\ \midrule
\textbf{Drums} & \GLS{WJD} & Automatic + manual & 67 & 6,506 & 28,851 cymbal onsets & Beat, chord, section \\
 & RWC-Jazz & Manual & 5 & 1,672 & N/A* & Beat, section \\
 & \textbf{\GLS{JTD} (ours)} & Automatic & 1,294 & 159,668 & 796,604 onsets & Beat \\
\bottomrule
\end{tabular}
\end{sidewaystable}

A summary of related datasets is given in Table~\ref{tab:jtd_existing_datasets}.

\subsection{Weimar Jazz Database}

The Weimar Jazz Database (\GLS{WJD}) contains note-for-note transcriptions of 456 improvised jazz solos \citep{Pfleiderer2017}. The note-level (pitches, onsets, offsets, intonation, and dynamics) annotations are aligned to the original audio, and the database also includes annotations of chord sequences, beat, and measure numbers. In the initial release of the database, the majority of the annotations were created manually: only dynamics and intonation were measured automatically. 

Subsequent projects have extended the \GLS{WJD} to include automatic transcriptions of both the bass \citep{Abesser2017} and drum \citep{Dittmar2018} accompaniments. However, these are only partial annotations --- consisting respectively of beat-wise pitch transcriptions and the ``swing ratios'' played by the drummer on their ride cymbal --- which may limit their usability in future work.

\subsection{FiloSax and FiloBass}

The FiloSax dataset \citep{Foster2021} is a collection of 24 hours of annotated jazz performances by five tenor saxophonists, recorded against a pre-recorded backing of piano, bass, and drums for 48 ``standard'' jazz compositions. Note-level annotations were created automatically for each recording using an algorithm and through manual transcription. The location of sections and timestamps were also annotated, enabling (for instance) the relative timing of the soloist to the accompaniment to be estimated.

FiloBass \citep{Riley2023} expands Filosax with transcriptions of the bassist's performance on each backing track. A source separation model was first applied to isolate this from the audio mixture, and a commercial automatic transcription algorithm was used to generate MIDI from this signal. FiloBass represents an important effort to use recent developments in music source separation to facilitate downstream annotation, which we expand upon here by providing annotations for every member in an ensemble.

\subsection{PiJAMA}\label{sec:jtd_pijama_description}

Piano Jazz with MIDI Annotations (\GLS{PiJAMA}) consists of 200+ hours of solo jazz piano performances transcribed using a pitch-to-MIDI algorithm \citep{Edwards2023}. The methodology used by the \GLS{PiJAMA} authors has several similarities with our contribution: curated playlists of relevant music were developed by scraping discographic services and audio was downloaded from YouTube. Although comprehensive, \GLS{PiJAMA} only covers solo piano recordings, while the authors acknowledge that it is far more common for jazz pianists to perform in a trio with bass and drums. In Chapter~\ref{chap:xai_rsi}, we combine a subset of \GLS{PiJAMA} with \GLS{JTD} to create a large dataset usable in jazz performer identification.

\subsection{RWC-Jazz}

Five of the 50 recordings contained in the RWC-Jazz database \citep{Goto2002} feature a line-up of piano, bass, and drums, with a total duration of 28 minutes. To the best of our knowledge, this is the only database with complete annotations provided for every musician in a jazz ensemble. These annotations were created manually and consist of MIDI data for each instrument, alongside beat, downbeat, and section timestamps.

\section{Database Curation}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/jtd/figure_1.png}
  \caption[Process followed for constructing \GLS{JTD}.]{Process followed for constructing \GLS{JTD}. Arrow block symbols indicate stages where tracks/artists may be removed.}
\label{fig:jtd_construction}
\end{figure}

When compiling any database of music recordings, deciding which material to include can be challenging. To simplify the process, we first identify a body of suitable ensembles, and then identify appropriate recordings from their discography to include in \GLS{JTD}. Our database curation and annotation procedure is shown in Figure~\ref{fig:jtd_construction}.

\subsection{Performer Selection}

For an artist to be included in \GLS{JTD}, they must be both ``popular'' and ``prolific''. With relation to ``popularity'', we want to ensure that artists are only included if they are both representative of general jazz listening habits and are highly regarded by experts. With relation to ``prolificacy'', we want to ensure that artists are only included if they have recorded a significant amount of material in the piano-bass-drums trio format.

\subsubsection{Identifying ``Popular'' Performers}\label{sec:jtd_popular_performers}

We search the Last.fm platform to obtain an overview of jazz listening habits. Last.fm is an online recommendation service that allows users to build profiles of their personal taste by tracking the music they listen to across many streaming platforms. We scrape the names of the top 250,000 performers and groups on Last.fm most frequently tagged by users with the genre ``Jazz'', using their official API.\footnote{\url{https://www.last.fm/api}} We order by tag count, rather than by plays or favourites, as we want to find the most quintessentially ``jazz'' artists, rather than those who fused jazz with other styles. 

\begin{figure}[!ht]
  \centering
  \includesvg[width=0.75\textwidth]{figures/jtd/figure_2}
  \caption[Total streams (``scrobbles'') of all recordings made by the top 20 ``trio'' artists most-frequently tagged as ``Jazz'' on Last.fm.]{Total streams (``scrobbles'') of all recordings made by the top 20 ``trio'' artists most-frequently tagged as ``Jazz'' on Last.fm.}
\label{fig:jtd_total_streams}
\end{figure}

From the resulting list of performers and groups, we select only those with the word ``Trio'' in their name. This leaves 249 unique performers or groups. While the names of many of the ``usual suspects'' feature prominently in this list (e.g., Bill Evans, Keith Jarrett), it also includes several performers who are mainly known in other genres (e.g., Dee Felice, who accompanied soul singer James Brown when he performed jazz standards) or who mostly composed soundtrack or ``stock'' music (e.g., Vince Guaraldi, famous for the soundtrack to ``A Charlie Brown Christmas'': see Figure~\ref{fig:jtd_total_streams}).

We then cross-reference the Last.fm results against two major jazz textbooks, keeping only those artists that receive a mention in the discographies of either Ted Gioia's ``The History of Jazz'' \citeyearpar{Gioia2011} or Mark Levine's ``The Jazz Piano Book'' \citeyearpar{Levine2011}. The intention here is to capture artists regarded highly by jazz experts. We note, however, that the retrospective nature of these textbooks could exclude performers who have become active only in recent years (or, indeed, in the time since their publication). Future revisions of \GLS{JTD} may use subsequent editions of both textbooks. 

This narrows the total number of groups to 33, all of which are named after a single musician (e.g., the Oscar Peterson Trio). This musician would have both led the ensemble and typically would be expected to compose the majority of the compositions they would play. Two bandleaders are bassists (Dave Holland and Ray Brown) and the remainder are pianists; no bandleaders are drummers.

\subsubsection{Identifying ``Prolific'' Performers}

Next, we turn to searching the MusicBrainz service to acquire a more detailed summary of each bandleader's recorded discography. MusicBrainz is a community-driven service that provides a comprehensive and open index of discographical metadata (including artist names, recording locations, and release dates) and is commonly used in music information retrieval tasks. We scrape MusicBrainz using their API to gather metadata relating to every individual recording ever made by each of our 33 bandleaders.\footnote{\url{https://musicbrainz.org/doc/MusicBrainz_API}} This results in the identification of 18,504 recordings.

We remove recordings that (1) are duplicated across several releases (for instance, those that also appeared on compilation albums), (2) do not contain a complete trio line-up, or that include multiple musicians performing on one instrument (for example, a piano ``four hands'' recording), (3) feature a musician doubling on a second instrument (for instance, a pianist who also plays synthesiser, or a drummer who plays auxiliary percussion), and (4) contain keywords in their title that suggested that they are incomplete performances (e.g., ``breakdown'', ``out-take'', ``false start'').

Four of the 33 bandleaders recorded less than an hour of material in the trio setting; amongst these, Dave Brubeck, Count Basie and Joe Zawinul are all better known for their work leading larger ensembles, while Art Tatum worked mainly in a trio of piano, bass, and guitar. After omitting the recordings by these bandleaders, 4,659 tracks remain.

\subsection{Recording Selection}

\subsubsection{Inclusion Criteria}\label{sec:jtd_inclusion_criteria}

To obtain a consistent set of tracks that our automated pipeline can analyse reliably, we define inclusion criteria to compare against each recording before including it in \GLS{JTD}. A track must have: (1) an approximate tempo between 100 and 300 quarter-note beats-per-minute (BPM), assessed by tapping along to the opening measures of the performance, (2) a clear quarter note pulse, with a time signature of either three or four beats per measure (and with no changes in meter), (3) an identifiable piano solo, accompanied by bass and drums with no interruptions in the ensemble texture (i.e., ``solo breaks''), (4) an uninterrupted ``swing eighths'' rhythmic feel, and (5) a link to the recording on YouTube.

Additional criteria are specified for each instrument in the trio. During their solo, pianists must have played on acoustic instruments (rather than, e.g., synthesiser or Fender ``Rhodes'' piano), and without any external FX (e.g., reverb, distortion). Bassists must also have played on an unmanipulated acoustic instrument using their fingers or a plectrum, not a bow. Drummers must have used a traditional ``traps'' kit (snare and kick drums; multiple cymbals, including hi-hat, ride, and crash; tom-toms), without auxiliary percussion (e.g., shakers, maracas). This must also have been played using sticks, rather than wire brushes or cloth mallets. 

These requirements are set so as to (1) ensure that all notes have clear onset times, and (2) maximise the acoustic similarity between the recordings in the dataset and the distribution of (mostly pop \& rock) material typically used to train audio source separation models. 

\subsubsection{Metadata Curation}\label{sec:jtd_metadata_curation}

For the recordings that met the inclusion criteria, we combine the metadata scraped from MusicBrainz with additional fields compiled manually, including timestamps for the beginning and ending of the piano solo, the position of individual instrument sources across the stereo spectrum, and the time signature. YouTube URLs are scraped from the ListenBrainz service using the track identifiers obtained from MusicBrainz and are manually corrected in the case of any false positives. Audio is downloaded from the URLs, trimmed to the piano solo, and stored in a lossless format with a sample rate of 44.1 kHz.

We elect only to include audio from piano solos as this is typically the only section in a performance where every musician would be expected to improvise. For instance, in the ``head'' (melodic statements that occur at the beginning and ending of ``straight ahead'' jazz performances), the material is pre-composed; while, in bass or drum solos, the accompanying musicians may choose not to play \citep{Monson1996}.

\begin{figure}[!ht]
  \centering
  \includesvg[width=0.75\textwidth]{figures/jtd/figure_3}
  \caption[Duration of piano solo excerpts by all bandleaders.]{Duration of piano solo excerpts by all bandleaders. Bar colour indicates subset (either \GLS{JTD} or \GLS{JTD-300}).}
\label{fig:jtd_solo_durations}
\end{figure}

Of the 4,659 recordings we evaluate, 1,294 (27.8\%) are included in \GLS{JTD}. The combined duration of all piano solos in these recordings is 44.5 hours (Figure~\ref{fig:jtd_solo_durations}). These recordings were made between 1947 and 2015, with a median recording year of 1972. The vast majority (93\%) have a time signature of four beats per measure.

\begin{figure}[!ht]
  \centering
  \includesvg[width=0.75\textwidth]{figures/jtd/figure_4}
  \caption[Number of tracks featuring the 10 performers on each instrument with the most recordings in \GLS{JTD}.]{Number of tracks featuring the 10 performers on each instrument with the most recordings in \GLS{JTD}.}
\label{fig:jtd_tracks_by_performer}
\end{figure}

The recordings in \GLS{JTD} feature 34 different pianists, 98 bassists, and 106 drummers. Bassist Ray Brown and drummer Ed Thigpen performed together as a rhythm section on the largest number of individual recordings (94). Of those who did not also lead their own trio, bassists Ron Carter and Sam Jones accompanied the largest number of different pianists (7), while drummer Roy Haynes accompanied 6 pianists. Figure~\ref{fig:jtd_tracks_by_performer} shows the number of tracks by the ten performers on each instrument with the greatest number of recordings in \GLS{JTD}.

\subsubsection{JTD-Brushes}

Excluding recordings where the drummer used wire brushes results in the removal of 973 tracks scraped from MusicBrainz. To address this imbalance, we create a separate set of annotations for tracks that are only rejected from \GLS{JTD} because of the use of brushes. We define this related database as JTD-Brushes. Note that the recordings in JTD-Brushes are not curated to the same extent as those in \GLS{JTD}; the full recording is used, rather than only the piano solo, and the time signature is estimated by the beat tracker automatically, rather than being provided beforehand. The duration of the recordings in JTD-Brushes is 79 hours. While the remainder of this article focuses on the curated \GLS{JTD}, we envisage that JTD-Brushes may be of use in developing or evaluating e.g., beat tracking or ``soft'' onset detection models \citep[e.g., ][]{Tomczak2023} that can cope with a broader variety of drumming techniques.

\subsection{Class Imbalance}\label{sec:jtd_class_imbalance}

A side effect of our inclusion criteria is that bandleaders who worked most frequently in the acoustic ``swing'' style are overrepresented in the \GLS{JTD}. Conversely, those who worked across a variety of styles or who frequently incorporated electronic elements into their music had very few recordings that met the inclusion criteria. The three bandleaders with the most audio in the \GLS{JTD} are Bill Evans (5.5 hours), Keith Jarrett (4), and Oscar Peterson (3.5). Meanwhile, Abdullah Ibrahim, Brad Mehldau, and Dave Holland are each represented in the \GLS{JTD} by less than half an hour of audio.

This class imbalance could prove problematic when training predictive models on the \GLS{JTD}. To that end, we create a subset of \GLS{JTD} (defined as \GLS{JTD-300}) that includes an equal number of recordings from the ten bandleaders with the most audio in \GLS{JTD}, sampled chronologically across the duration of their careers. Together, recordings led by these ten musicians --- all of whom are pianists --- make up 62.6\% of the total duration of audio in \GLS{JTD} (see Figure~\ref{fig:jtd_solo_durations}).

For each of these bandleaders, we sort their tracks in \GLS{JTD} chronologically by the date of recording. In cases where a full date cannot be obtained from MusicBrainz, a track is estimated to have been made either midway through the month (in cases where both a month and year are provided) or year (when only a year is provided) it was recorded. In cases where multiple dates are given for one track (i.e., when an album was recorded over a period of time, without dates being assigned to individual tracks), we take the midpoint of these dates. If no dates are provided at all, the track is excluded from selection.

\begin{figure}[!ht]
  \centering
  \includesvg[width=1.0\textwidth]{figures/rsos_rhythm/figure_1}
  \caption[Duration of the recording career for each of the ten pianists included in \GLS{JTD-300}]{Duration of the recording career for each of the ten pianists included in \GLS{JTD-300}. Markers indicate the date of recordings that appear in \GLS{JTD-300}, randomly jittered horizontally and vertically for visual clarity.}
\label{fig:rsos_jtd300}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includesvg[width=1.0\textwidth]{figures/rsos_rhythm/figure_s1}
  \caption[Durations of recordings in \GLS{JTD-300}.]{Durations of recordings in \GLS{JTD-300}. Each column shows the total duration of the recordings by every bandleader with recordings in \GLS{JTD-300}, with the green proportion corresponding with those tracks that are included within each subset (shown also as a percentage along the columns).}
\label{fig:rsos_bandleader_proportions}
\end{figure}

Next, we sort each track into one of 30 equally spaced bins; the left edge of the first bin coincides with the date of a bandleader's earliest recording, and the right edge of the final bin with the day of their final (or most recent) recording. Tracks are ordered within each bin by the proximity of their recording date to the midpoint of that bin; if multiple recordings were made on the same day, they are arranged following the order in which they appeared on their original release. We then include the first track from each bin in \GLS{JTD-300}: when an individual bin is empty, we add an additional track from the first bin, then the second bin, and so on, until 30 tracks are obtained for every bandleader (Figure~\ref{fig:rsos_jtd300}). We show the proportion of recordings by each bandleader contained in \GLS{JTD-300} within Figure~\ref{fig:rsos_bandleader_proportions}.

\begin{figure}[!ht]
  \centering
  \includesvg[width=0.75\textwidth]{figures/jtd/figure_5}
  \caption[Histogram grouping number of tracks in \GLS{JTD} by year of recording.]{Histogram grouping number of tracks in \GLS{JTD} by year of recording. Bar colour indicates subset (\GLS{JTD} or \GLS{JTD-300}.}
\label{fig:jtd_tracks_by_year}
\end{figure}

The combined duration of the audio in \GLS{JTD-300} is 11.75 hours (26.4\% of the duration of \GLS{JTD}). As with \GLS{JTD}, these recordings were made between 1947 and 2015, with a median recording year of 1977.5. The distribution of recording years included in \GLS{JTD-300} is comparable with \GLS{JTD} (Figure~\ref{fig:jtd_tracks_by_year}). 95\% of \GLS{JTD-300} recordings have a time signature of four beats per measure. We envisage that \GLS{JTD-300} will most likely be of use in performer identification tasks or analyses of chronological trends in improvisation style (here, see Chapter~\ref{chap:rhythm_rsos}). Tasks that do not require balanced class sizes may be better accomplished using the full \GLS{JTD}.

\section{Annotation}

\subsection{Source Separation}

\subsubsection{Model Selection}

The \texttt{ZFTurbo} audio source separation model \citep{Solovyev2024} is used to separate each track in the \GLS{JTD} into four sources; ``bass'', ``drums'', ``vocals'', and ``other''. This model combines three existing source separation models and achieved the first prize in a recent community sound demixing competition \citep{Fabbro2024}. 

\subsubsection{Audio Channel Selection}

Before the development of full-spectrum stereophonic panning in the 1970s, audio signals could only be panned to the left or right speaker output, or both (centre channel). For tracks in the \GLS{JTD} where at least one instrument in the trio was hard-panned in this manner (39\%), source separation is applied independently to the left and right monaural signals alongside the centre output. The required channel is identified manually for each instrument during the metadata curation process (see Section~\ref{sec:jtd_metadata_curation}).

\subsubsection{Audio Preprocessing}\label{sec:jtd_audio_preprocessing}

As the \texttt{ZFTurbo} model was not trained explicitly to separate piano audio, we use the ``other'' source (consisting of the residual audio after removing every other source from the mixed signal) for this instrument. As a result, it is possible that audio from the bass and drums could ``bleed'' into this track, which would affect the performance of our downstream annotation pipeline.

To mitigate this possibility, prior to annotation we filter the ``other'' source using a second-order Butterworth bandpass filter. Our goal is to attenuate frequency bands that could also be occupied by the drums and bass, while minimising any damage to the quality of the piano audio. The filter is set to attenuate frequencies outside the range of 55--3520 Hz, equivalent to a six-octave span from $A_1$--$A_7$: as shown in \citet{Edwards2023}, the vast majority of note events in jazz piano performance typically fall within this range. 

We demonstrate in Section~\ref{sec:jtd_alternate_methods} that filtering the piano audio improves the accuracy of our onset annotation pipeline compared with using the raw audio, and that this particular frequency range performs better than an alternative, quantitatively ``narrower'' range.

\subsubsection{``Vocal'' Source}

Another possibility is that the unmixed ``vocal'' source could also contain piano audio, which would necessitate summing it with the ``other'' source. We manually inspect the ``vocal'' source across a sample of 34 recordings sampled from \GLS{JTD-300} (see Section~\ref{sec:jtd_ground_truth_annotations}). In all of these cases, the ``vocal'' source is either silent or contains vocalisations of the musicians (or, occasionally in the case of a live recording, members of the audience) reacting to the performance. Therefore, the ``vocal'' source is not used during downstream data extraction.

\subsection{Data Extraction}

Our data extraction pipeline consists of three components: (1) an onset detection algorithm, applied to each separated audio source (except ``vocal''), (2) a beat and downbeat tracking algorithm, applied to the raw audio mixture, and (3) an algorithm to match onsets with their nearest beat, in order to assign a metrical interpretation to each onset. Our implementations of (1) and (2) are from the \texttt{madmom} (version \texttt{0.17.0}) Python package \citep{Bock2016}. The default setting of 100 frames per second is used, such that near-simultaneous events with a time difference of less than one frame (likely including, e.g., piano chords) are treated as a single onset.

\subsubsection{Onset Detection}\label{sec:jtd_onset_detection}

The \texttt{CNNOnsetDetector} function in \texttt{madmom} is used to detect onsets in the isolated audio obtained for each instrument in the trio. This algorithm came first in the most recent \texttt{MIREX} onset detection competition.\footnote{\url{https://nema.lis.illinois.edu/nema_out/mirex2018/results/aod/}\label{note:mirex_2018}} A convolutional neural network (\GLS{CNN}) is fed spectrogram frames of audio (at the default rate of 100 frames per second) to obtain an onset activation function. This envelope is then smoothed using a moving window and onsets are identified as local maxima higher than a predetermined threshold. Both the window size and the peak-picking thresholds are optimised separately for each instrument class in \GLS{JTD} (see Section~\ref{sec:jtd_validation_results}). In Section~\ref{sec:jtd_alternate_methods}, we demonstrate that, for detecting onsets in the \GLS{JTD} piano audio, this algorithm outperforms both an existing automatic piano transcription algorithm \citep{Kong2021} and a naive spectral flux approach.

\subsubsection{Beat Detection}\label{sec:jtd_beat_detection}

The \texttt{DBNDownBeatTracker} function in \texttt{madmom} is used to detect the position of quarter note beats and downbeats in the audio mixture. This algorithm came first in the most recent \texttt{MIREX} beat tracking competition.\footnote{\url{https://nema.lis.illinois.edu/nema_out/mirex2019/results/abt/smc/}} A recurrent neural network (\GLS{RNN}) is applied to frames from an audio spectrogram to distinguish between beats, downbeats, and non-beat classes; a dynamic Bayesian network (\GLS{DBN}) then infers meter from the \GLS{RNN} observations (using the default rate of 100 frames per second). Meter changes cannot be detected by this algorithm, so we incorporate this into our inclusion criteria (see Section~\ref{sec:jtd_inclusion_criteria}).\footnote{\citet{Foscarin2024} have since proposed an end-to-end algorithm that does not require \GLS{DBN} post-processing of an activation function, which alleviates constraints on meter while potentially leading to greater overall precision. This was not available at the time of compiling \GLS{JTD}, however.}

We apply this algorithm multiple times to a single track. First, the default parameters specified by the authors are used, with the tempo allowed to vary between 100 and 300 quarter note beats-per-minute (as specified in the inclusion criteria, Section~\ref{sec:jtd_inclusion_criteria}). This results in frequent tempo changes, due to the lack of constraint in the parameter settings. 

To resolve this, we gradually reduce the range of permissible tempo values for the beat tracker over a number of iterations. First, we calculate the durations between successive beats as
\begin{align}
d_{i} = b_{i+1} - b_{i}, \ i = 1, 2, \dots, N-1, 
\end{align}
where $N$ is the total number of beats in the track. Then, we calculate the inter-quartile range of beat durations $IQR_d$ as
\begin{align}
IQR_d &= Q_3\left( \{d_1, d_2, \dots, d_{N-1}\} \right) - Q_1\left( \{d_1, d_2, \dots, d_{n-1}\} \right),
\end{align}
and use this to filter the initial durations $d$, where 
\begin{align}
d'_i &= \{ Q_1 - 1.5(IQR_d) \leq d_i \leq Q_3 + 1.5(IQR_d) \}, \ i = 1, 2, \dots, N-1.
\end{align}
Finally, we recalculate $Q_1$ and $Q_3$ using only values of $d'$ and set these as the new minimum and maximum tempo for the beat tracker in the next iteration of the process. We optimise the total number of iterations for this process specifically for the recordings in \GLS{JTD} (see Section~\ref{sec:jtd_validation}).

The downbeat classes estimated from the \GLS{RNN} are then combined with the detected beat positions and the time signature for the recording, in order to assign measure numbers to every detected timestamp. 

\subsubsection{Beat-Onset Matching}\label{sec:jtd_beat_onset_matching}

The final stage of our detection pipeline involves matching every beat with the nearest onset detected in each instrumental source. The size of the window we use to match any given onset to the nearest beat varies depending on the tempo of a track. Onsets played up to one thirty-second note before and one sixteenth note after any given timestamp are included within the window; whichever onset has the smallest absolute distance to the beat is understood to mark the pulse. If no onsets are contained within a window, then the musician is considered to not have marked the pulse at that timestamp. 

Our rationale for using an asymmetric window comes from previous research which has suggested that jazz soloists are more likely to ``lag behind'' and mark the beat later than their rhythm section, as opposed to earlier \citep{Butterfield2010}. Additionally, \citet{Corcoran2021} describe how notes played more than a thirty-second note earlier than a given beat typically act as a subdivision of the previous beat (i.e., a delayed, swung ``eighth'' note), as opposed to marking that beat itself. 

\subsection{Validation}\label{sec:jtd_validation}

\subsubsection{Ground Truth Annotations}\label{sec:jtd_ground_truth_annotations}

To evaluate the effectiveness of our annotation pipeline, we manually annotate beat, downbeat, and onset positions across a representative subsample of tracks in \GLS{JTD}. In particular, we want to check whether the pipeline is resilient to a range of possible recording qualities, and also to the styles of the musicians represented most prominently in \GLS{JTD}. Consequently, we adopt a systematic approach, annotating the earliest, middle, and final recordings made by each of the ten bandleaders in \GLS{JTD-300}. Owing to his over representation in \GLS{JTD}, we also annotate an additional four tracks by Bill Evans (selected randomly from \GLS{JTD-300}) --- enough to ensure that the percentage of recordings led by Evans in the subsample (20.6\%) is equivalent to \GLS{JTD} (19.6\%).

The combined duration of the 34 recordings in the subsample is 72 minutes. The median recording year is 1975, comparable with both \GLS{JTD} and \GLS{JTD-300} (see Sections~\ref{sec:jtd_metadata_curation} and~\ref{sec:jtd_class_imbalance}), and the range of recording years is identical (1947--2015). 88\% of recordings have a time signature of 4 quarter notes per measure. 

\subsubsection{Validation Results}\label{sec:jtd_validation_results}

The performance of the pipeline is determined by considering the proportion of detected annotations that occur within a 50 ms window of a ground truth annotation. For every track in the subsample, separate precision, recall, and $F$-measures are computed for each class: (1--3) the onsets within each unmixed instrumental source, and (4) the quarter note beats and (5) downbeats within the audio mixture. When $F = 1.0$, every onset or beat detected by the pipeline can be matched with a ground truth annotation, with no onsets or beats unmatched. 

We apply the gradient-free, non-linear optimisation algorithm \texttt{subplex} \citep{Rowan1990} implemented in the \texttt{NLopt} (version \texttt{2.7.1}) Python package to set the parameters of the beat and onset detection algorithms separately for each audio source.\footnote{\url{https://github.com/stevengj/nlopt}} In every case, the mean value of $F$ across the entire hand-annotated subsample is treated as the objective function to maximise. Note that, for the beat detection algorithm, only the $F$-measure obtained from evaluating the beat timestamps --- and not the downbeats --- is considered during optimisation.

\begin{table}[H]
\centering
\caption[Optimised results for onset detection per instrument]{Optimised results for onset detection per instrument, showing mean $F$-measure, precision, and recall. Brackets show \emph{SD}s.}
\label{tab:jtd_optimization_results}
\begin{tabular}{l c c c}
\toprule
\textbf{Instrument} & \textbf{F-measure} $\uparrow$ & \textbf{Precision} $\uparrow$ & \textbf{Recall} $\uparrow$ \\
\midrule
\textbf{Piano} & 0.93 (0.03) & 0.93 (0.04) & 0.93 (0.04) \\
\textbf{Bass} & 0.93 (0.05) & 0.94 (0.04) & 0.93 (0.07) \\
\textbf{Drums} & 0.95 (0.03) & 0.96 (0.04) & 0.94 (0.04) \\ \midrule
\textbf{Beats} & 0.97 (0.05) & 0.97 (0.05) & 0.97 (0.05) \\
\textbf{Downbeats} & 0.63 (0.44) & 0.63 (0.44) & 0.63 (0.44) \\
\bottomrule
\end{tabular}
\end{table}

We show the values of $F$ obtained from the optimised parameter set in Table~\ref{tab:jtd_optimization_results}. The performance of the onset detection algorithm is broadly equivalent across instrument classes, with drums performing marginally better than bass and piano. In comparison with earlier results obtained from applying the same algorithm to equivalent multi-tracked audio datasets (``polyphonic pitched'', $\text{mean}(F) = 0.95$, ``plucked strings'', $\text{mean}(F) = 0.90$, ``solo drums'', $\text{mean}(F) = 0.93$, see Footnote~\ref{note:mirex_2018}), we obtain similar onset detection accuracy. This indicates that the source separation process is unlikely to have caused significant damage (in the form of artifacts, etc.) to the audio, to the point that onset detection algorithms trained on multi-tracked recordings cannot also be applied to \GLS{JTD}.

While the beat tracking results indicate good performance, downbeat tracking is noticeably worse. To understand why, we inspect all tracks where $F(downbeat) = 0.0$, while $F(beat) > 0.95$ (i.e., the downbeats are out-of-phase). We observe that, in the majority of these cases, metrically ``strong'' beats are mistaken for other ``strong'' beats (i.e., beat 2 and beat 4, in 4/4), rather than ``weak'' beats, and vice versa. This suggests that our results may still have some value in the analysis of metrical structure. Our results are also in line with those obtained from applying the same algorithm to other metrically complex forms of music.\footnote{\url{https://www.music-ir.org/mirex/wiki/2016:Audio_Downbeat_Estimation_Results}} Moreover, they reflect general challenges involved in music downbeat detection \citep[see][]{Durand2016}. However, future revisions of \GLS{JTD} may leverage the modular design of our codebase to integrate more sophisticated downbeat tracking algorithms \citep[e.g.,][]{Foscarin2024}, as these become available.

Finally, we consider the temporal difference between equivalent automatic and ground truth annotations. The pipeline tends to annotate beats and onsets earlier than the human annotators; the mean difference in beat location time (algorithm--human) is $-4.82$ ms ($SD = 4.58$), and for onsets the mean difference is $-4.53$ ms ($SD = 3.42$). There are no substantial differences between onsets detected for different instrument classes here (piano, $-5.77$ ms, bass: $-4.29$ ms, drums: $-4.53$ ms). In context, this variation is likely perceptually sub-threshold, and should be negligible in most applications.

\subsubsection{Alternative Methods}\label{sec:jtd_alternate_methods}

Next, we compare several alternative onset detection methods, using the piano audio and corresponding ground truth annotations for evaluation. These methods are: (1) an automatic transcription model \citep{Kong2021}, which generates MIDI (incorporating both pitch and rhythmic information) from audio, (2) a naive audio signal processing approach, whereby onsets are identified as local maxima from a normalised spectral flux envelope, using the implementation from the \texttt{librosa} (version \texttt{0.8.1}) Python package \citep{McFee2015}, (3) the current approach, without the audio filtering described in Section~\ref{sec:jtd_audio_preprocessing}, and (4) the current approach, with a quantitatively ``narrower'' bandpass filter (attenuating frequencies outside the range of 110--1760 Hz i.e., $A_2$--$A_6$).

Note that, as method (1) generates multiple MIDI events for what would be regarded perceptually as a single onset (i.e., a chord), near-simultaneous MIDI events within a window of 50 ms are grouped together, with the onset of the earliest event taken as the onset of the whole group. The size of this window is optimised following the method described in Section~\ref{sec:jtd_validation_results}, as are the individual parameter sets used to pick peaks from the activation functions returned separately from methods (2--4). The default setting of 100 frames per second is used for method (1).

\begin{table}[H]
\caption[Onset detection results per method, piano only]{Onset detection results per method, piano only.}
\label{tab:jtd_methods_comparison}
\centering
\begin{tabular}{l c c c}
\toprule
\textbf{Method} & \textbf{F-measure} $\uparrow$ & \textbf{Precision} $\uparrow$ & \textbf{Recall} $\uparrow$ \\
\midrule
\textbf{Table \ref{tab:jtd_optimization_results}} & \textbf{0.93} (0.03) & 0.93 (0.04) & \textbf{0.93} (0.04) \\ \midrule
\textbf{\citet{Kong2021} (1)} & 0.77 (0.13) & 0.71 (0.16) & 0.86 (0.09) \\
\textbf{Spectral Flux (2)} & 0.84 (0.06) & 0.79 (0.10) & 0.90 (0.04) \\
\textbf{Ours, no filter (3)} & 0.92 (0.03) & 0.90 (0.06) & \textbf{0.93} (0.03) \\
\textbf{Ours, narrow filter (4)} & 0.92 (0.03) & \textbf{0.95} (0.03) & 0.89 (0.05) \\
\bottomrule
\end{tabular}
\end{table}

The results from this comparison are given in Table~\ref{tab:jtd_methods_comparison}. For detecting onsets in the \GLS{JTD} piano audio, the current approach outperforms all four alternatives. In particular, we note how ``narrowing'' the bandpass filter seems to affect the precision-recall trade-off of the CNN predictions: the narrower filter setting (4), for example, results in a lower false positive rate than the settings described in Section~\ref{sec:jtd_audio_preprocessing}, but a higher false negative rate. The current approach, with filter settings between those of methods (3--4), results in approximately equal precision and recall. All three CNN-based methods significantly outperform spectral flux.

The results obtained for method (1) are disappointing, especially comparing with those given by the authors. One possibility is that this model may have overfitted to the acoustic properties of the instrument(s) in its training data and be less resilient to the varied recording conditions contained in \GLS{JTD}. More robust approaches that make use of e.g., augmented training data may be worth exploring in the future \citep[here, see][]{Edwards2024}.

Regardless, these MIDI annotations are included in \GLS{JTD}, with the caveat that they may be of greater use in downstream tasks which do not require strict timing accuracy (e.g., in analysing melodic or harmonic content). The total number of MIDI events is 2,174,833 ($\sim2.5$x the number of total piano onsets).

\subsubsection{Bass Annotation}\label{sec:jtd_bass_detection}

Considering the results in Table~\ref{tab:jtd_optimization_results}, the  higher standard deviation for the bass onset detection results suggests that there may be a subset of onset types that give outlier results, diminishing the overall accuracy of the pipeline. While we cannot investigate every possibility, we can consider three hypotheses: whether bass annotation is negatively affected (1) by if a bass note is ``raked'', (2) by if both bass and drums play simultaneously, and (3) on specific beats of the bar.

For hypothesis (1), we define a ``rake'' as a technique in jazz bass where the finger is dragged from one string to the next lower string, usually with a triplet rhythm. We collect all groups of three bass onsets that are immediately preceded and then followed by a single onset that matched with the quarter-note pulse. Searching all tracks with ground truth annotations, we find 57 occurrences of this pattern, equivalent to 285 onsets. 

For each track where this pattern occurs at least once, we compute the $F$-measure between all of the automatically detected ``rake'' patterns and ground truth onsets located within the span of each ``rake'' (allowing for a tolerance of 50 ms). From this analysis, $\text{mean}(F) = 0.91$ ($SD = 0.08$) --- i.e., close to the value in Table~\ref{tab:jtd_optimization_results}. This suggests that whether a bass note is ``raked'' does not have a substantial impact on the accuracy of the detection pipeline.

For hypothesis (2), we collect all automatically detected bass onsets that are matched with a timestamp from the beat tracker, provided that a drum onset can also be matched with that same timestamp ($N = 10,538$). The equivalent ground truth annotations are collected and, again, $\text{mean}(F)$ is computed only for these annotations. In this case, $\text{mean}(F) = 0.92\;(SD = 0.06)$) --- again, nearly identical to the value obtained using all annotations. This suggests that the accuracy of onset detection is not substantially lower when both bass and drums play together on the same beat.

Finally, for hypothesis (3), we collect automatic and manual onset annotations matched with each individual beat of the ground truth metre annotations. $F$-scores are then computed for each individual beat (i.e., comparing all known beat 1s, all beat 2s, etc.) across every track. Here, $\text{mean}(F_{B1}) = 0.94\;(SD=0.07)$, $\text{mean}(F_{B2}) = 0.94\;(SD=0.12)$, $\text{mean}(F_{B3}) = 0.95\;(SD=0.06)$, and $\text{mean}(F_{B4}) = 0.95\;(SD=0.04)$. All results are again in line with Table~\ref{tab:jtd_optimization_results}, which suggests that there is no evidence that outlying results occur systematically on individual beats of the bar.

\section{Analysis}

Across \GLS{JTD}, there are 2,206,413 onsets (piano: 866,116, bass: 543,693, drums: 796,604) and 512,272 beat timestamps, 130,547 of which are labelled as downbeats. On average, a solo contains 669 piano onsets, 420 bass onsets, 616 drums onsets, 396 beats, and 101 downbeats. 87.4\% of beat timestamps can be matched with a drum onset, and 82.0\% matched with a bass onset; only 71.0\% of beats could be matched with a piano onset, which underlines how keeping time is typically not the primary aim of jazz soloists.

\subsection{Solo Duration}

On average, the duration of a piano solo in \GLS{JTD} is 2 minutes and 3 seconds. Both the shortest and longest solos are by Bud Powell, with the shortest lasting only 22 seconds (``Salt Peanuts'', 1956), and the longest 7 minutes, 17 seconds (``Reets And I'', 1962).

\begin{figure}[!ht]
  \centering
  \includesvg[width=0.75\textwidth]{figures/jtd/figure_6}
  \caption[Mean duration of solos by different \GLS{JTD} pianists.]{Mean duration of solos by different \GLS{JTD} pianists. Error bars show standard errors.}
\label{fig:jtd_mean_track_duration_by_pianist}
\end{figure}

There is considerable variation in the duration of solos played by different pianists (Figure~\ref{fig:jtd_mean_track_duration_by_pianist}). In particular, the later a pianist's first recording in \GLS{JTD} was made (i.e., the later they began their career), the longer they tended to solo for. For the 9 (out of 27) pianists in \GLS{JTD} with the longest average solo durations, the median year their first recording was made is 1967; for the 9 pianists with the shortest solos on average, the median year is 1956. 

One possible explanation is that the open-ended formal designs (e.g., modal ``vamps'', ``time, no changes'') that became popular in jazz during the latter half of the twentieth century proved more suitable for extended improvisation, as this did not need to follow the harmonic structure of an underlying ``song form'' \citep{Gioia2011}. Changing trends in the music business could provide another explanation; up until the mid-1950s, record companies would often fill albums with multiple shorter recordings rather than longer individual takes, so that individual tracks could be released as singles to be played on jukeboxes and radio stations \citep{Priestley1991}.

\subsection{Tempo}\label{sec:jtd_tempo}

Rather than working directly with the timestamps estimated from the beat tracker, we instead define the location of a ``beat'' as the mean of all onsets matched to a single timestamp (see Section~\ref{sec:jtd_beat_onset_matching}). Given our omission of tracks with, e.g. a predominant ``two-beat'' feel (where half notes could conceivably be understood to mark the ``beat'') and with ``straight'' eighths (where these could be felt to mark the beat), these timestamps should correspond with quarter notes. In cases where fewer than two onsets are matched with a single timestamp (for instance, where at least two instruments played ``off'' that beat), that beat is set to missing. At least two instruments played ``on'' the beat for 88.7\% of detected beat timestamps within \GLS{JTD}.

\begin{figure}[!ht]
  \centering
  \includesvg[width=0.75\textwidth]{figures/jtd/figure_7}
  \caption[Distribution of tempi in \GLS{JTD}.]{Distribution of tempi in \GLS{JTD}, shown in quarter note beats-per-minute).}
\label{fig:jtd_mean_tempo}
\end{figure}

The mean tempo $\bar{y}$ of a recording in beats-per-minute is calculated as 
\begin{align}\label{eq:mean_tempo}
\bar{y} = \dfrac{\sum\limits_{i=1}^N\frac{60}{x_i - x_{i-1}}}{N-1}
\end{align}
where $x_i$ is the mean of all onsets matched to beat $i$, for $i \geq 1$. Across \GLS{JTD}, the observed $\text{mean}(\bar{y}) = 193.92 \ \text{BPM}$ ($SD = 46.93$, Figure~\ref{fig:jtd_mean_tempo}); this is comparable to the equivalent result calculated using the timestamps $y'$ estimated from the beat tracking algorithm ($\text{mean}(\bar{y'}) = 193.70, SD = 46.59$). The slowest performance in \GLS{JTD} has a mean tempo of 102.10 BPM (Cedar Walton, ``Ghost of a Chance'', 2010) and the fastest 299.82 BPM (John Hicks, ``Airegin'', 1987).

As a measure of tempo stability, for each track we obtain the standard deviation of the tempo normalised by the mean tempo i.e., the percentage fluctuation about the overall tempo (``tempo fluctuation'', $F$). This can be written simply as
\begin{align}\label{eq:tempo_fluctuation}
F = \dfrac{\sqrt{\frac{1}{N-1} \sum\limits_{i=1}^N (y_i - \bar{y})^2}}{\bar{y}},
\end{align}
where $y_i$ is the tempo value in quarter-note beats-per-minute at beat $i$. We observe $\text{mean}(F) = 5.55\%$ ($SD = 2.70$) for \GLS{JTD} recordings, suggesting that they are generally stable.

We also calculate the slope of a linear regression of instantaneous tempo against beat onset time (``tempo slope'', $S$) using the standard least squares formula
\begin{align}\label{eq:tempo_slope}
\hat{S} = \frac{\sum\limits_{i=1}^N (x_i - \bar{x}) (y_i - \bar{y})}{\sum\limits_{i=1}^N (x_i - \bar{x})^2},
\end{align}
such that $\hat{S} > 0$ implies acceleration and $\hat{S} < 0$ implies deceleration. We observe $\text{mean}(\hat{S}) = 0.04\ \text{BPM/s}$ ($SD = 0.10$) for \GLS{JTD} recordings, suggesting that they display a very slight tendency towards acceleration (with a predicted increase of 1 BPM every 25 seconds).

There is no correlation between signed tempo slope and unsigned tempo fluctuation, $r_{\hat{S}, \ F}(1292) = -0.06, p = 0.06$; however, there is a positive correlation between the unsigned slope (i.e., any global change in tempo, regardless of direction) and fluctuation, $r_{|\hat{S}|, \ F}(1292) = 0.37, p < .001$. There are positive correlations between both mean tempo and signed tempo slope, $r_{\bar{y}, \ \hat{S}}(1292) = 0.25, p < .001$, mean tempo and unsigned slope, $r_{\bar{y}, \ |\hat{S}|}(1292) = 0.60, p < .001$, and mean tempo and fluctuation, $r_{\bar{y}, \ F}(1292) = 0.24, p < .001$. This suggests that performances which have a faster pace tend to involve a greater magnitude of tempo change than those which are slower, and that these performances are also less stable overall.

\subsection{Swing}\label{sec:jtd_swing}

In jazz, swing refers to the division of the pulse into alternating long and short intervals. Expressed in Western notation, the long interval is typically written as a quarter note triplet, and the short as an eighth note triplet. Empirically, swing can be measured by taking the ratio of these long and short durations --- the swing or ``beat-upbeat ratio'' (\GLS{BUR}), commonly expressed in binary logarithmic form in the literature. This can be written simply as
\begin{align}\label{eq:jtd_bur}
log_2(BUR)=log_2\biggl(\frac{t_{a,b}-t_{a}}{t_{b}-t_{a,b}}\biggr),
\end{align}
where $t_a$ is the onset matched with beat $a$, $t_b$ is the onset matched with beat $b$ (i.e., at beat $a + 1$), and $t_{a,b}$ is the single onset between beats $a$ and $b$. For notated ``swung'' eighths, $log_2(BUR) = 1.0$ ($BUR = 2:1$); vice-versa, for ``straight'' eighths (i.e., the equal-equal subdivision of the beat), $log_2(BUR) = 0.0$ ($BUR = 1:1$).

We search our database for all discrete groupings of three onsets where the first and last mark the quarter note pulse. The total number of such groupings is 417,278. Following the analysis of the \GLS{WJD} by \citet{Corcoran2021}, we classify ratios above 4:1 ($log_2(BUR) = 2$) and below 1:4 ($log_2(BUR) = -2$) as outliers, which results in the exclusion of 2.10\% of these groupings. The final number of beat-upbeat ratios in the dataset is 408,506 (piano: 151,640, bass: 47,115, drums: 209,751).

\subsubsection{Differences in Swing Between Instruments}\label{sec:jtd_differences_in_swing_between_instruments}

To evaluate the differences in swing ratio between the instruments in the trio, we smooth the distribution of beat-upbeat ratios obtained for each instrument through kernel density estimation (calculating the bandwidth using Silverman's rule-of-thumb). Then, we apply the peak-picking algorithm (using the default parameters) from the \texttt{SciPy} (version \texttt{1.10.1}) Python library \citep{Virtanen2020} to obtain the local maxima of the smoothed curve. Confidence intervals for these peaks are generated by bootstrapping over the results from different bandleaders ($N = 10,000$ replicates).

\begin{sidewaysfigure}[ph!]
  \includesvg[width=1.0\textwidth]{figures/jtd/figure_8}
  \caption[Distribution of $log_2(BUR)$s between instruments across \GLS{JTD}]{Distribution of $log_2(BUR)$s between instruments across \GLS{JTD}, normalised such that the height of the largest bar in each panel is 1. Dotted vertical lines show peaks of the density estimates; straight lines correspond with the musical notation given along the top of the panel.}
\label{fig:jtd_bur_distribution_by_instrument}
\end{sidewaysfigure}

For the piano, we find one peak in the density estimate, corresponding with $log_2(BUR)$ of 0.37 ($BUR = 1.44:1$, 95\% CI: $[0.15, 0.43]$). For the bass, we find two peaks, corresponding with $log_2(BURs)$ of 0.06 ($1.06:1$, $[0.03, 0.13]$) and 1.23 ($3.41:1$, $[1.15, 1.27]$). For the drums, we also find two peaks, at $log_2(BUR) = -0.91$ ($0.40:1$, $[-1.15, -0.70]$) and 1.18 ($3.25:1$, $[1.13, 1.23]$). These density estimates and the corresponding peaks are shown in Figure~\ref{fig:jtd_bur_distribution_by_instrument}.

This analysis suggests that (1) all instrument classes primarily target long-short divisions of the beat, with the percentage of total $log_2(BURs) > 0.0 = 88.6\%$ (piano: 81.9\%, bass: 89.1\%, drums: 93.4\%); (2) pianist beat-upbeat ratios are closer to notated ``straight'' than ``swung'' eighths, judging by the peak of their density estimate; (3) in contrast, the larger of the two peaks in the bass and drums density estimates suggest that both instruments tend towards higher beat-upbeat ratios than would be implied by notated swing eighths alone. 

The fact that soloists tend to swing less than the expected $2:1$ ratio --- while accompanists swing ``more'' than this --- has been well documented. \citet{Butterfield2011} has, for instance, suggested that relatively ``straight'' eighth notes help to maintain forward momentum in a soloist's improvisation, while the more ``swung'' eighth notes of the accompaniment help to facilitate hierarchical beat perception.

We also note that the mean beat-upbeat ratio for piano soloists in \GLS{JTD} ($1.53:1$) is close to that obtained for \texttt{SWING} tracks in \GLS{WJD} \citep[$1.41:1$; see][]{Corcoran2021}. Caution should be taken when interpreting these results, however, as \GLS{WJD} contains a greater variety of tempi and solo instruments than \GLS{JTD}.

\subsubsection{Effect of Tempo on Swing}\label{sec:jtd_effect_of_tempo_on_swing}

Next, we consider the relationship between swing and the tempo of a performance. We fit a linear mixed effects model using the implementation in the \texttt{statsmodels} (version \texttt{0.13.1}) Python library \citep{Seabold2010}. We predict a performer's mean $log_2(BUR)$ using the mean tempo of the recording (standardised through $z$-transformation), their instrument, and the interaction between tempo and instrument as fixed effects (with piano as the reference category). Bandleader is used as a random effect (slopes and intercepts). An individual musician's performance is excluded if 15 ratios could not be obtained, which results in 471 individual performances out of 3,865 being excluded (piano: 30, bass: 401, drums: 40).

\begin{figure}[!ht]
  \centering
  \includesvg[width=0.75\textwidth]{figures/jtd/figure_9}
  \caption[Mean $log_2(BUR)$ and tempo for an individual recording and instrument in \GLS{JTD}.]{Mean $log_2(BUR)$ and tempo for an individual recording and instrument in \GLS{JTD}. Solid lines show predictions (without random effects), shaded areas show 95\% confidence intervals (obtained via bootstrapping over data from different pianists, $N = 10,000$)}
\label{fig:jtd_bur_vs_tempo}
\end{figure}

An increase in mean tempo is a significant predictor of a decrease in mean beat-upbeat ratio for a recording (Figure~\ref{fig:jtd_bur_vs_tempo}), with a one $SD$ change in BPM associated with a decrease of -0.14 mean $log_2(BUR)$ ($p < .001$, 95\% CI: $[-0.16, -0.12]$). This suggests that it becomes harder for musicians to articulate long-short subdivisions of the quarter note as its duration decreases. This ``straightening'' effect has also been observed in analyses of both \GLS{WJD} \citep{Corcoran2021} and Filosax \citep{Foster2021}.

There are significant interactions between instrument and tempo for both the bassist ($\beta= -0.06, p < .001$, 95\% CI: $[-0.09, -0.04]$) and drummer ($\beta= -0.10, p < .001$, 95\% CI: $[-0.12, -0.08]$). Put differently, the ``straightening'' effect is more severe for accompanying (rather than soloist) roles. The amount of variance in the data explained by both the fixed and random effects of the model is 58.8\%, compared with 61.9\% for the fixed effects only. This suggests only minimal differences in the effect of tempo on swing between ensembles.

\subsubsection{Effect of Recording Year on Swing}

We fit the model described in Section~\ref{sec:jtd_effect_of_tempo_on_swing} with an additional fixed effect of recording year (standardised via $z$-transformation) and the interaction between year and tempo. Increases in recording year have no significant main effect on mean $log_2(BUR)$ ($\beta = 0.01, p = 0.44$, 95\% CI: $[-0.02, 0.05]$), and there is no significant interaction between year and tempo ($\beta < 0.01, p = 0.79$, 95\% CI: $[-0.01, 0.01]$). Including the year of recording in the model also results in minimal change to the amount of variance it explains (conditional $R_2 = 62.8\%$, marginal $R_2 = 59.3\%$).

This suggests that the year a track was recorded has minimal impact on the levels of swing displayed by the musicians. These results should not be taken as indicative of global trends in swing timing, however, as the recordings in \GLS{JTD} are mostly representative of the ``straight ahead'' jazz style, as practised throughout the last century. Prior analysis of the \GLS{WJD} has demonstrated tangible differences in swing between jazz styles \citep{Corcoran2021}.

\subsubsection{Comparison with Existing Datasets}\label{sec:jtd_dataset_comparison}

Finally, we want to compare the levels of ``swing'' demonstrated in \GLS{JTD} with existing open-source datasets of annotated jazz performances. We select \GLS{WJD} for comparison to the \GLS{JTD} piano recordings and FiloBass for comparison to the \GLS{JTD} bass recordings. We scrape all tracks from both databases that meet the tempo and style inclusion criteria for \GLS{JTD}.

For \GLS{WJD}, this means that we include tracks where \texttt{instrument == "piano"}, \\ \texttt{rhythmfeel == "SWING"}, and $100 < \texttt{avgtempo} < 300$, using the metadata provided in the \texttt{solo\_info} table. Likewise, for FiloBass we include tracks that are tagged as \texttt{"Swing"}, have either a 3/4 or 4/4 time signature, and where the provided downbeat annotations suggest a tempo between 100 and 300 BPM. This leaves 6 out of 456 recordings in \GLS{WJD} and 44 out of 48 FiloBass tracks. No tracks are duplicated across the three datasets; however, some of the same performers do appear, including bassists Ron Carter and Ray Drummond in FiloBass and \GLS{JTD}. 

\begin{sidewaysfigure}[ph!]
  \includesvg[width=1.0\textwidth]{figures/jtd/figure_x1}
  \caption[Distribution of $log_2(BUR)$s for recordings in \GLS{WJD} and FiloBass]{Distribution of $log_2(BUR)$s for recordings in \GLS{WJD} and FiloBass, following the format in Figure~\ref{fig:jtd_bur_distribution_by_instrument}.}
\label{fig:jtd_bur_distribution_by_dataset}
\end{sidewaysfigure}

We then match onset annotations from both \GLS{WJD} and FiloBass with their nearest annotated quarter-note beats and extract \GLS{BUR}s. Only downbeat annotations are provided for FiloBass, however, so we annotate the remaining beats between successive downbeat timestamps through linear interpolation according to the time signature of the recording. As outlined in Section~\ref{sec:jtd_swing}, we discard \GLS{BUR}s outside the range of $-2 < log_2(BUR) < 2$. We extract 707 \GLS{BUR}s for \GLS{WJD} and 3,218 for FiloBass. We show normalised kernel density and peak estimates for the $log_2(BUR)$ distributions in both datasets in Figure~\ref{fig:jtd_bur_distribution_by_dataset}, following the process outlined in Section~\ref{sec:jtd_differences_in_swing_between_instruments}.

With regards to the piano, the distribution of $log_2(BUR)$s in \GLS{WJD} is broadly equivalent to \GLS{JTD}, with one peak found at $log_2(BUR) = 0.05$ ($1.05:1$). This is considerably lower than the equivalent peak in \GLS{JTD} ($log_2(BUR) = 0.37, 1.44:1$). Inspecting this, we find that the mean tempo of the six piano recordings in \GLS{WJD} (275.63 BPM) is much higher than across \GLS{JTD} (193.92). As we have already shown in Figure~\ref{fig:jtd_bur_vs_tempo}, higher tempi lead to demonstrably ``straighter'' \GLS{BUR} values.

With regards to the bass, we find two peaks in the distribution of \GLS{BUR}s obtained from FiloBass, at $log_2(BUR) = -0.22$ ($0.80:1$) and $log_2(BUR) = 1.23$ ($3.41:1$). These are broadly similar to the two peaks found in \GLS{JTD} (see Figure~\ref{fig:jtd_bur_distribution_by_instrument}). However, we note that the proportion of ``straight'' bass \GLS{BUR}s (where $log_2(BUR)\approx0$) is significantly higher in FiloBass than in \GLS{JTD}. One possibility is that the recordings in FiloBass are ``play-along'' backing tracks (as opposed to commercial recordings), meaning that there was no soloist actually improvising during the recording session. Another possibility is simply increased variance in the data, due to the smaller number of recordings in FiloBass versus \GLS{JTD}.

\subsection{Synchronisation}\label{sec:jtd_synchrony}

Synchrony can be defined as the difference between onsets that demarcate the same musical event (e.g., a quarter note beat). While it can be expressed in ``raw'' units (milliseconds, frames), we express synchrony as a percentage of the duration of a single quarter note at the tempo of a track, which allows for comparison across performances made at different tempi. For example, a value of $+25\%$ implies that one musician plays a sixteenth note later than another. 

\begin{figure}[!ht]
  \centering
  \includesvg[width=0.75\textwidth]{figures/jtd/figure_10}
  \caption[Kernel density estimates for the relative position of beats by each instrument in \GLS{JTD}]{Kernel density estimates for the relative position of beats by each instrument in \GLS{JTD}, indicated by colour. Density estimates are scaled such that the maximum height of the curve for each instrument is 1.}
\label{fig:jtd_propeller_plot}
\end{figure}

In Figure~\ref{fig:jtd_propeller_plot}, we show kernel density estimates for the relative position of beats by each instrument on a circle: the flow of time unfolds in a clockwise direction, with values shifted such that the mean position of drummers' first beat aligns with 0 degrees.

We calculate the synchrony between all pairs of instruments in the trio at every quarter note, across all tracks in \GLS{JTD}. Confidence intervals are again obtained by bootstrapping over the results obtained for different bandleaders ($N = 10,000$). Bassists play 1.5\% (95\% CI: $[1.0, 2.0]$) of a quarter note later than drummers, on average. This close synchronisation between bass and drums helps anchor the jazz soloist's performance \citep{Butterfield2010}. On average, pianists play 5.6\% (95\% CI: $[4.8, 6.3]$) of a quarter note later than drummers, and 4.0\% ($[3.2, 4.9]$) later than bassists; approximately a sixty-fourth note (6.3\%) delay. 

This delay between soloist and accompaniment has been observed frequently in the literature on jazz improvisation \citep[e.g.,][]{Butterfield2010}. To investigate whether it depends on the tempo of a performance, we fit a mixed effects model that predicts the mean asynchrony between pianist and accompaniment. As fixed effects, we use the tempo ($z$-transformed), the accompanying instrument (bass or drums), and the interaction between tempo and accompanying instrument (bass = reference category). Bandleader is used as a random effect (slopes and intercepts).

Increased tempo predicts significantly reduced asynchrony; for every $SD$ increase in tempo, mean pianist-accompaniment asynchrony decreases by $-0.4\%$ of the duration of a quarter note beat ($p < .05$, 95\% CI: $[-0.7, -0.1]$). There is a significant interaction between accompanying instrument and tempo ($\beta = 0.3, p < .01$, 95\% CI: $[0.1, 0.5]$). Faster performances thereby have tighter soloist-accompaniment synchronisation than slower ones, with this effect being stronger for piano-bass synchronisation than piano-drums.\footnote{It is also important to be aware of how, at faster tempo values, the slight variation in onset detection accuracy between instrument classes could also have had an impact on these results. At the mean tempo in \GLS{JTD}, the 1.5 ms difference between bass and piano annotation is equivalent to 0.5\% of the duration of a quarter note.}

The size of this effect is relatively small, however, with less than a 256th note difference (1.56\% of a quarter note beat) in predicted mean asynchrony between pianist and bassist at both the slowest and fastest tempo in the corpus. The amount of variation in the data explained by the fixed effects is only 5.6\%, compared with 29.3\% for fixed and random effects --- suggesting that differences between ensembles could be a more likely source of variation.

\section{Discussion}

In this chapter, we have presented Jazz Trio Database (JTD), a new dataset of 44.5 hours of jazz piano trio recordings with automatically generated annotations. Appropriate recordings were identified by scraping user-based listening and discographic data, a source separation model was applied to isolate audio for piano, bass, and drums, and annotations were generated by applying various automatic transcription, onset detection, and beat tracking algorithms to the separated audio. Onsets detected by the pipeline achieved a mean $F$-measure of 0.94 when compared with ground truth annotations. 
	
We encourage the use of \GLS{JTD} in tasks including performer identification, expressive performance modelling, symbolic music generation, music structure analysis (using the piano solo timestamps), and as a benchmark to compare audio signal processing algorithms. JTD-Brushes may also prove useful in developing e.g., beat tracking models that can cope with a broader variety of drumming styles and ``soft'' onsets. Comparison between accompanied and unaccompanied jazz piano playing may be another fruitful direction for research (here, see Section~\ref{sec:rsi_feature_importance_by_dataset}). The lower accuracy of the automated downbeat annotations compared with both onsets and beats could also inspire the development of downbeat trackers better suited for the jazz genre.

We can foresee some limitations of our work. Our inclusion criteria were particularly strict, which necessitated identifying tracks manually. Automated tagging methods (e.g., distinguishing between a drummer's use of brushes or sticks) could enable more efficient data collection. \GLS{JTD} also shows an imbalance towards male performers; exceptional inclusions could be made in future revisions to include prolific female musicians who did not appear in the Last.fm search results or jazz textbooks. Finally, we do not include MIDI transcriptions of the bass and drums as we do for the piano, but this offers an exciting opportunity for future work as models capable of performing these tasks continue to develop in sophistication.

Many of the recordings contained in \GLS{JTD} are under copyright and cannot be released publicly. However, we make both the mixed and unmixed (i.e., source-separated) \GLS{JTD} audio files downloadable upon request, on a third-party archive.\footnote{\url{https://zenodo.org/records/13828030}} The design of our code is modular, enabling \GLS{JTD} to be updated easily as the state-of-the-art in the various tasks involved in the annotation pipeline improves.\footnote{\url{https://github.com/HuwCheston/Jazz-Trio-Database}} 

We have also prepared a number of interactive web applications that enables \GLS{JTD} to be explored without having to download it. The first application presents an interactive, graph-like interface showing the different musicians in \GLS{JTD-300}: every performer (piano, bass, drums) is shown as a node on a graph, and connections show performers who have previously recorded together.\footnote{\url{https://huwcheston.github.io/Jazz-Trio-Database/resources/trio-network-search.html}} The second application shows a variety of features extracted from recordings in the full \GLS{JTD}: these can be sorted based on the values of each feature, and recordings can be clicked on to show additional graphs and figures.\footnote{\url{https://huwcheston.github.io/Jazz-Trio-Database/resources/data-explorer.html}}

\subsection{Recent Developments}\label{sec:jtd_subsequent_developments}

Since the initial publication of \GLS{JTD}, a number of extensions to and applications of the database have been proposed by members of the \GLS{MIR} community. 
\begin{itemize}
\item[--]{The lead author of the FiloBass dataset \citep{Riley2023} has applied their transcription pipeline to the source-separated recordings in \GLS{JTD}, with the intention of providing accurate, automatic transcriptions in MIDI format of the entire jazz trio.\footnote{An example of these transcriptions can be listened to at \url{https://huwcheston.github.io/Jazz-Trio-Database/resources/xriley_kbarron_howdeepistheocean_render.mp3}}} 
\item[--]{The authors of the beat tracker described in \citet{Foscarin2024} have also fine-tuned their model for the jazz genre using the ground-truth annotations provided in \GLS{JTD} (see Section~\ref{sec:jtd_ground_truth_annotations}), with the aim of further optimising the results presented in Table~\ref{tab:jtd_optimization_results}.} 
\item[--]{The instrument labels and audio files in \GLS{JTD} were included as part of the dataset used to train Aria-CL, a \GLS{CNN} that identifies solo piano sections in audio recordings.\footnote{\url{https://github.com/loubbrad/aria-cl}} This model, in turn, has been employed to create a new dataset of automatically transcribed solo piano recordings in MIDI format, spanning many different musical genres \citep{Bradshaw2025}.}
\end{itemize}
We are encouraged to see these initial applications of \GLS{JTD} by the community and are excited by the possibilities it can unlock for future computational research, both in jazz and beyond.

To make working with \GLS{JTD} easier for the end user, we have also integrated it with \texttt{mirdata} \citep{Bittner2019}. This is an open-source Python library that provides a consistent API for interacting with common datasets used in \GLS{MIR}, alongside helping to facilitate reproducible experimentation.\footnote{\url{https://github.com/mir-dataset-loaders/mirdata}} The following example shows how the dataset can be downloaded and initialised from inside the Python interpreter with only a few lines of code:

\begin{minted}{python}
import mirdata

# Initialise the dataset and download the annotations
jtd = mirdata.initialize('jtd')
jtd.download()
\end{minted}

Next, we show how the metadata for a single performance can be loaded:
\begin{minted}{python}
# Choose a random multitrack
multi_track = jtd.choice_multitrack()

# Access properties of the performance
name = multi_track.name
tempo = multi_track.tempo
time_signature = multi_track.time_signature
print(name, tempo, time_signature)
\end{minted}

The final example loads MIDI from a single piano track into the \texttt{pretty\_midi} library (a common Python package for working with MIDI data) and plots a ``piano roll'' representation, similar to those that are used to train the models described in Sections~\ref{sec:rsi_representation_learning_approach} and~\ref{sec:rsi_factorised_inputs_approach}:

\begin{minted}{python}
import seaborn as sns
from pretty_midi import PrettyMIDI

# Access the piano MIDI
piano = multi_track.piano    # alternatively, .bass, .drums
midi = piano.midi

# Unpack the MIDI attributes from the mirdata representation
pitches = midi.pitches.astype(int)
starts, ends = midi.intervals[:, 0], midi.intervals[:, 1]
velocities = midi.confidence.astype(int)
all_data = zip(starts, ends, velocities, pitches)

# Convert to pretty_midi format
pm_notes = [
    Note(start=s, end=e, pitch=p, velocity=v) 
    for (s, e, p, v) in all_data
]
instrument = Instrument(program=0)
instrument.notes = pm_notes

# Compute the piano roll with the default frames-per-second and display
piano_roll = instrument.get_piano_roll(fs=100)
sns.heatmap(piano_roll)
\end{minted}

In the following chapters, we explore the ways in which \GLS{JTD} can be deployed to model the factors contributing towards improvisation style (Chapter~\ref{chap:xai_rsi}) and the use of rhythm in jazz (Chapter~\ref{chap:rhythm_rsos}).